\documentclass[10pt]{article}
\usepackage[margin=0.9in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{longtable}

\title{Physics-Constrained Split-Spline Neural Networks for \\
J-V Curve Reconstruction in Perovskite Solar Cells: \\
\large Comprehensive Technical Documentation (v3.0)}

\author{Technical Documentation for ICML Submission}
\date{}

\begin{document}
\maketitle

\tableofcontents
\newpage

%=============================================================================
\section{Introduction and Problem Statement}
%=============================================================================

This document provides comprehensive technical documentation for a physics-informed machine learning framework designed to predict current-voltage (J-V) characteristics of perovskite solar cells from material parameters. The framework bridges expensive COMSOL finite-element simulations with fast neural network inference, enabling rapid device optimization.

\subsection{Problem Formulation}

Given 31 material/device parameters $\mathbf{x} \in \mathbb{R}^{31}$ (bandgaps, mobilities, thicknesses, doping densities, interface recombination velocities), predict the complete J-V curve $\mathbf{J}(V) \in \mathbb{R}^{45}$ evaluated at a \textbf{fixed non-uniform voltage grid}.

\textbf{Sign convention}: We use positive photogenerated current density (mA/cm$^2$), where $J > 0$ for $V < V_{oc}$ and $J = 0$ at $V = V_{oc}$. This is the ``power-producing quadrant'' convention.

\subsection{Non-Uniform Voltage Grid (Critical Detail)}

The voltage grid is \textbf{non-uniform} with denser sampling near the knee region (MPP):

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
def build_voltage_grid():
    return np.concatenate([
        np.arange(0, 0.4 + 1e-8, 0.1),      # Region 1: 5 points, 0.1V spacing
        np.arange(0.425, 1.4 + 1e-8, 0.025) # Region 2: 40 points, 0.025V spacing
    ]).astype(np.float32)
\end{lstlisting}

\textbf{Grid structure}:
\begin{itemize}
    \item \textbf{Low voltage} ($0 \leq V \leq 0.4$ V): 5 points at $\Delta V = 0.1$ V
    \item \textbf{High voltage} ($0.425 \leq V \leq 1.4$ V): 40 points at $\Delta V = 0.025$ V
    \item \textbf{Total}: 45 points, denser in the MPP/knee region
\end{itemize}

This non-uniform sampling necessitates $\Delta V$-weighted metrics for evaluation.

\subsection{Feature Pipeline Summary}

From 31 raw input parameters, we compute \textbf{71 candidate physics-informed features}. \textbf{Train-only feature selection} (Pearson correlation filtering with threshold $|r| < 0.3$, followed by pairwise multicollinearity pruning at $|r| > 0.9$) reduces these to $m$ selected features, typically $m \approx 4$:
\begin{equation}
\mathbf{x}_{input} \in \mathbb{R}^{31 + m}, \quad m \approx 4 \text{ (after selection)}
\end{equation}

%=============================================================================
\section{Architecture \& Implementation Details}
%=============================================================================

\subsection{Overall System Architecture}

The framework's \textbf{main contribution} is the \textbf{UnifiedSplitSplineNet}: a multi-head neural network that jointly predicts physical anchors and region-specific control points, followed by a differentiable PCHIP reconstruction layer with hard physics projection.

\begin{equation}
\text{Input} \rightarrow \text{SharedBackbone} \rightarrow \begin{cases}
\text{Head A: Anchors } (J_{sc}, V_{oc}, V_{mpp}, J_{mpp}) \\
\text{Head B: Region 1 control points (}K\text{ points)} \\
\text{Head C: Region 2 control points (}K\text{ points)}
\end{cases} \rightarrow \Pi_{\mathcal{C}} \rightarrow \text{PCHIP} \rightarrow \hat{J}(V)
\end{equation}

\subsection{Shared Backbone Architecture (VocNN)}

The shared backbone uses a residual architecture with the following \textbf{exact specifications}:

\begin{table}[h]
\centering
\caption{Shared Backbone Architecture (Exact Configuration)}
\begin{tabular}{lccccc}
\toprule
\textbf{Layer} & \textbf{Input} & \textbf{Output} & \textbf{Activation} & \textbf{Norm} & \textbf{Dropout} \\
\midrule
Input Linear & $31 + m$ & 384 & GELU & BatchNorm1d & 0.1-0.3 \\
ResBlock 1 & 384 & 256 & GELU & BatchNorm1d & 0.1-0.3 \\
ResBlock 2 & 256 & 128 & GELU & BatchNorm1d & 0.1-0.3 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Residual Block Implementation}

Each ResBlock implements (with projection shortcut when dims differ):
\begin{equation}
\mathbf{y} = \text{Proj}(\mathbf{x}) + \text{Dropout}\left(\text{BN}\left(\sigma\left(\text{Linear}_2\left(\text{BN}\left(\sigma\left(\text{Linear}_1(\mathbf{x})\right)\right)\right)\right)\right)\right)
\end{equation}

where $\sigma \in \{\text{GELU}, \text{SiLU}\}$ (searched via HPO).

\subsubsection{Multi-Head Outputs}

From the 128-dimensional backbone:
\begin{itemize}
    \item \textbf{Anchor Head}: Linear(128 $\rightarrow$ 4) $\rightarrow$ $(J_{sc}, V_{oc}, V_{mpp}, J_{mpp})$
    \item \textbf{Region 1 Head}: Linear(128 $\rightarrow$ $K$) $\rightarrow$ Tanh $\rightarrow$ control points for $[0, V_{mpp}]$
    \item \textbf{Region 2 Head}: Linear(128 $\rightarrow$ $K$) $\rightarrow$ Tanh $\rightarrow$ control points for $[V_{mpp}, V_{oc}]$
\end{itemize}

Default $K = 4$ control points per region (HPO searches $K \in \{2, 4, 6, 8\}$).

\subsection{Jacobian-Informed Training (Optional)}

The framework includes \textbf{Jacobian regularization} for input-output sensitivity control, though it is \textbf{disabled by default} due to instability issues.

\subsubsection{Forward Pass with Jacobian}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
def forward_with_jacobian(self, x, voc_ceiling=None):
    """Computes output + Jacobian norm via Hutchinson's estimator."""
    x.requires_grad_(True)
    y = self.forward(x)

    # Hutchinson's trace estimator for Frobenius norm
    v = torch.randn_like(x)  # Random probe vector
    jacobian_vector = torch.autograd.grad(
        outputs=y, inputs=x, grad_outputs=v,
        create_graph=True, retain_graph=True
    )[0]
    jacobian_norm = (jacobian_vector ** 2).sum(dim=1).mean() / x.shape[1]

    return y, jacobian_norm
\end{lstlisting}

\subsubsection{Jacobian Loss Term}

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{MSE} + \lambda_{jac} \cdot \|\nabla_x f(\mathbf{x})\|_F^2 + \lambda_{phys} \cdot \mathcal{L}_{ceiling}
\end{equation}

where:
\begin{itemize}
    \item $\lambda_{jac} \in \{0, 10^{-5}, 10^{-4}\}$ (default: 0, \textbf{disabled})
    \item $\lambda_{phys} = 0$ (ceiling constraint disabled due to estimation issues)
    \item $\mathcal{L}_{ceiling} = \text{ReLU}(\hat{V}_{oc} - V_{oc}^{ceiling})^2$ (soft upper bound)
\end{itemize}

\textbf{Note}: HPO heavily biases toward $\lambda_{jac} = 0$ (3/5 suggestions are zero).

\subsection{Physics Constraint Enforcement}

\subsubsection{Feasible Set Definition}

\begin{equation}
\mathcal{C} = \left\{ (J_{sc}, V_{oc}, V_{mpp}, J_{mpp}) :
\begin{array}{l}
J_{sc} \geq \epsilon, \quad V_{oc} \geq \epsilon \\
\epsilon \leq V_{mpp} \leq V_{oc} - \epsilon \\
\epsilon \leq J_{mpp} \leq J_{sc} - \epsilon
\end{array}
\right\}, \quad \epsilon = 10^{-3}
\end{equation}

\subsubsection{Projection with Violation Tracking}

\begin{algorithm}[h]
\caption{Physics Projection $\Pi_{\mathcal{C}}$ with Violation Logging}
\begin{algorithmic}[1]
\REQUIRE Raw predictions $(\tilde{J}_{sc}, \tilde{V}_{oc}, \tilde{V}_{mpp}, \tilde{J}_{mpp})$
\STATE $\epsilon \leftarrow 10^{-3}$
\STATE \textbf{Log violations:}
\STATE \quad $v_1 \leftarrow \text{count}(\tilde{J}_{sc} < \epsilon)$ \COMMENT{Jsc negative}
\STATE \quad $v_2 \leftarrow \text{count}(\tilde{V}_{oc} < \epsilon)$ \COMMENT{Voc negative}
\STATE \quad $v_3 \leftarrow \text{count}(\tilde{V}_{mpp} \geq \tilde{V}_{oc})$ \COMMENT{Vmpp exceeds Voc}
\STATE \quad $v_4 \leftarrow \text{count}(\tilde{J}_{mpp} \geq \tilde{J}_{sc})$ \COMMENT{Jmpp exceeds Jsc}
\STATE \textbf{Project:}
\STATE $J_{sc} \leftarrow \max(\tilde{J}_{sc}, \epsilon)$
\STATE $V_{oc} \leftarrow \max(\tilde{V}_{oc}, \epsilon)$
\STATE $V_{mpp} \leftarrow \text{clamp}(\tilde{V}_{mpp}, \epsilon, V_{oc} - \epsilon)$
\STATE $J_{mpp} \leftarrow \text{clamp}(\tilde{J}_{mpp}, \epsilon, J_{sc} - \epsilon)$
\RETURN $(J_{sc}, V_{oc}, V_{mpp}, J_{mpp})$, violations $(v_1, v_2, v_3, v_4)$
\end{algorithmic}
\end{algorithm}

\subsection{Split-Spline PCHIP Reconstruction}

\subsubsection{Knot Construction with Monotonicity Guarantee}

Knot voltages via cumulative sigmoid scaling:
\begin{equation}
V_{knot,i} = V_{min} + \sum_{j \leq i} \sigma(\alpha_j) \cdot \frac{V_{max} - V_{min}}{K+1}, \quad \sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

Knot currents via cumulative subtraction:
\begin{equation}
J_{knot,i} = J_{anchor} - \sum_{j \leq i} \sigma(\beta_j) \cdot \Delta J_{max}
\end{equation}

This guarantees: (a) strictly increasing voltages, (b) non-increasing currents.

\subsubsection{PCHIP Slope Computation (Fritsch-Carlson)}

For monotone data $(x_i, y_i)$, slopes $d_i$ computed as:
\begin{equation}
d_i = \begin{cases}
0 & \text{if } \delta_{i-1} \cdot \delta_i \leq 0 \\
\frac{3(h_{i-1} + h_i)}{\frac{h_{i-1} + 2h_i}{\delta_{i-1}} + \frac{2h_{i-1} + h_i}{\delta_i}} & \text{otherwise}
\end{cases}
\end{equation}
where $h_i = x_{i+1} - x_i$ and $\delta_i = (y_{i+1} - y_i)/h_i$. This preserves monotonicity by construction \cite{fritsch1980monotone}.

%=============================================================================
\section{Hyperparameter Optimization}
%=============================================================================

\subsection{HPO Framework Configuration}

\begin{table}[h]
\centering
\caption{Optuna HPO Configuration}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
\midrule
Sampler & TPE (multivariate) & Tree-structured Parzen Estimator \\
Pruner & Hyperband & min\_resource=10, reduction\_factor=4 \\
n\_trials (NN) & 200 & VocNN, DirectCurveNet \\
n\_trials (LGBM) & 300 & All LGBM models \\
n\_startup\_trials & 75 & Random before TPE \\
n\_parallel\_trials & 24 & Parallel workers \\
timeout\_per\_model & 14400s & 4 hours \\
\bottomrule
\end{tabular}
\end{table}

\subsection{VocNN Hyperparameter Search Space}

\begin{table}[h]
\centering
\caption{VocNN HPO Search Space (Exact Ranges)}
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Range} & \textbf{Distribution} & \textbf{Default} \\
\midrule
\multicolumn{4}{l}{\textit{Architecture}} \\
n\_layers & [2, 3] & Integer & 3 \\
hidden\_dim\_0 & \{256, 384, 512\} & Categorical & 384 \\
hidden\_dim\_1+ & \{128, 256\} & Categorical & 256, 128 \\
activation & \{gelu, silu\} & Categorical & gelu \\
\midrule
\multicolumn{4}{l}{\textit{Regularization}} \\
dropout & [0.1, 0.3] & Uniform & 0.2 \\
jacobian\_weight & \{0, 0, 0, $10^{-5}$, $10^{-4}$\} & Categorical & 0 \\
physics\_weight & 0 & Fixed (disabled) & 0 \\
\midrule
\multicolumn{4}{l}{\textit{Optimization}} \\
lr & [$10^{-4}$, $2 \times 10^{-3}$] & Log-uniform & $10^{-3}$ \\
weight\_decay & [$10^{-6}$, $10^{-4}$] & Log-uniform & $10^{-5}$ \\
epochs & [150, 300] & Integer & 200 \\
patience & 30 & Fixed & 30 \\
use\_amp & True & Fixed & True \\
\bottomrule
\end{tabular}
\end{table}

\subsection{LightGBM Hyperparameter Search Space}

\begin{table}[h]
\centering
\caption{LGBM HPO Search Space (Exact Ranges)}
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Range} & \textbf{Distribution} & \textbf{Notes} \\
\midrule
\multicolumn{4}{l}{\textit{Tree Structure}} \\
num\_leaves & [31, 255] & Integer & Controls complexity \\
max\_depth & [6, 15] & Integer & Tree depth limit \\
min\_child\_samples & [10, 50] & Integer & Leaf minimum \\
min\_child\_weight & [$10^{-3}$, 1.0] & Log-uniform & \\
\midrule
\multicolumn{4}{l}{\textit{Learning}} \\
learning\_rate & [0.01, 0.1] & Log-uniform & \\
n\_estimators & [500, 2000] & Integer & Boosting rounds \\
\midrule
\multicolumn{4}{l}{\textit{Sampling}} \\
subsample & [0.7, 0.95] & Uniform & Row sampling \\
subsample\_freq & [1, 5] & Integer & \\
colsample\_bytree & [0.7, 0.95] & Uniform & Column sampling \\
feature\_fraction & [0.7, 0.95] & Uniform & \\
bagging\_fraction & [0.7, 0.95] & Uniform & \\
bagging\_freq & [1, 5] & Integer & \\
\midrule
\multicolumn{4}{l}{\textit{Regularization}} \\
reg\_alpha & [$10^{-4}$, 1.0] & Log-uniform & L1 \\
reg\_lambda & [$10^{-4}$, 1.0] & Log-uniform & L2 \\
min\_split\_gain & [0.0, 0.1] & Uniform & \\
path\_smooth & [0.0, 0.5] & Uniform & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{DirectCurveShapeNet HPO Search Space}

\begin{table}[h]
\centering
\caption{DirectCurveShapeNet HPO Search Space}
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Range} & \textbf{Distribution} & \textbf{Notes} \\
\midrule
\multicolumn{4}{l}{\textit{Architecture}} \\
n\_layers & [2, 5] & Integer & Network depth \\
hidden\_dim\_0 & [256, 768] & Integer & First hidden \\
hidden\_dim\_1 & [128, 512] & Integer & Second hidden \\
hidden\_dim\_2+ & [64, 256] & Integer & Later layers \\
dropout & [0.05, 0.3] & Uniform & \\
activation & \{silu, gelu, relu\} & Categorical & \\
ctrl\_points & [6, 12] & Integer & Spline control \\
use\_residual & \{True, False\} & Categorical & \\
\midrule
\multicolumn{4}{l}{\textit{Loss Weights}} \\
lr & [$10^{-5}$, $10^{-2}$] & Log-uniform & \\
weight\_decay & [$10^{-7}$, $10^{-3}$] & Log-uniform & \\
weight\_smooth & [0.01, 0.2] & Log-uniform & Smoothness penalty \\
weight\_mono & [0.5, 2.0] & Uniform & Monotonicity penalty \\
knee\_weight & [1.0, 4.0] & Uniform & MPP region weight \\
huber\_delta & [0.05, 0.3] & Uniform & Huber loss $\delta$ \\
\bottomrule
\end{tabular}
\end{table}

%=============================================================================
\section{Feature Engineering (71 Physics Features)}
%=============================================================================

From 31 raw COMSOL parameters, we derive 71 physics-informed features organized into functional groups.

\subsection{Input Parameters (31 dimensions)}

\begin{table}[h]
\centering
\caption{Raw Input Parameters from COMSOL}
\begin{tabular}{lll}
\toprule
\textbf{Category} & \textbf{Parameters} & \textbf{Units} \\
\midrule
Band edges (6) & $\chi_{ETL}^{e,h}$, $\chi_{Perov}^{e,h}$, $\chi_{HTL}^{e,h}$ & eV \\
Mobilities (6) & $\mu_n^{ETL,P,HTL}$, $\mu_p^{ETL,P,HTL}$ & cm$^2$/Vs \\
Thicknesses (3) & $l_{ETL}$, $l_{Perov}$, $l_{HTL}$ & nm \\
Lifetimes (2) & $\tau_e$, $\tau_h$ & s \\
Doping (4) & $N_c^{ETL,P}$, $N_v^{HTL,P}$ & cm$^{-3}$ \\
Recombination (4) & $v_{II}$, $v_{III}$, $B_{rad}$, $C_{Aug}$ & cm/s, cm$^3$/s \\
Permittivity (3) & $\varepsilon_{ETL}$, $\varepsilon_{Perov}$, $\varepsilon_{HTL}$ & - \\
Work functions (2) & $W_{anode}$, $W_{cathode}$ & eV \\
Generation (1) & $G_{avg}$ & cm$^{-3}$s$^{-1}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Derived Physics Features}

\subsubsection{A. Energetics (7 features)}
\begin{align}
E_g &= \chi_{Perov}^h - \chi_{Perov}^e & \text{(Perovskite bandgap)} \\
V_{bi} &= W_{anode} - W_{cathode} & \text{(Built-in voltage)} \\
E_g^{offset} &= E_g - V_{bi} & \text{(Schottky vs Ohmic indicator)}
\end{align}

\subsubsection{B. Interface Barriers (9 features)}
\begin{align}
\Delta E_v^{HP} &= \chi_{HTL}^h - \chi_{Perov}^h & \text{(VB offset HTL/Perov)} \\
\Delta E_c^{PE} &= \chi_{ETL}^e - \chi_{Perov}^e & \text{(CB offset Perov/ETL)} \\
\Phi_{h,ETL} &= \chi_{ETL}^h - \chi_{Perov}^h & \text{(Hole blocking at ETL)} \\
\Phi_{e,HTL} &= \chi_{HTL}^e - \chi_{Perov}^e & \text{(Electron blocking at HTL)}
\end{align}

\subsubsection{C. Transport - Diffusion Lengths (8 features)}
\begin{align}
L_n &= \sqrt{\mu_n^P \cdot \tau_e \cdot k_B T / q} & \text{(Electron diffusion length)} \\
L_p &= \sqrt{\mu_p^P \cdot \tau_h \cdot k_B T / q} & \text{(Hole diffusion length)} \\
\eta_{coll,e} &= L_n / l_{Perov} & \text{(Electron collection efficiency)} \\
\eta_{coll,h} &= L_p / l_{Perov} & \text{(Hole collection efficiency)}
\end{align}

\subsubsection{D. $\mu\tau$ Products (4 features)}
\begin{align}
\mu\tau_e &= \mu_n^P \cdot \tau_e & \text{(Electron transport quality)} \\
\mu\tau_h &= \mu_p^P \cdot \tau_h & \text{(Hole transport quality)} \\
\mu\tau_{balance} &= \mu\tau_e / \mu\tau_h & \text{(Transport balance)}
\end{align}

\subsubsection{E. Drift/Extraction - Theta Parameters (6 features)}

The Theta parameter represents the extraction figure of merit from Hecht equation:
\begin{align}
\mathcal{E}_{bi} &= V_{bi} / l_{Perov} & \text{(Built-in field)} \\
\Theta_e &= \frac{\mu_n^P \cdot \tau_e \cdot V_{bi}}{l_{Perov}^2} & \text{(Electron extraction FOM)} \\
\Theta_h &= \frac{\mu_p^P \cdot \tau_h \cdot V_{bi}}{l_{Perov}^2} & \text{(Hole extraction FOM)} \\
\Theta_{min} &= \min(\Theta_e, \Theta_h) & \text{(Limiting extraction)}
\end{align}

\subsubsection{F. Series Resistance Estimates (5 features)}
\begin{align}
\sigma_{HTL} &= q \cdot \mu_p^{HTL} \cdot N_v^{HTL} \\
R_s^{HTL} &= l_{HTL} / \sigma_{HTL} \\
R_s^{total} &= R_s^{ETL} + R_s^{Perov} + R_s^{HTL}
\end{align}

\subsubsection{G. Generation (4 features)}
\begin{align}
J_{max} &= q \cdot G_{avg} \cdot l_{Perov} & \text{(Maximum theoretical $J_{sc}$)} \\
G_{per\_nm} &= G_{avg} / l_{Perov} & \text{(Generation density)}
\end{align}

\subsubsection{H. Recombination (8 features)}
\begin{align}
\tau_{eff} &= \left(\frac{1}{\tau_e} + \frac{1}{\tau_h}\right)^{-1} & \text{(Effective lifetime)} \\
R_{SRH} &\propto \frac{1}{\tau_e + \tau_h} & \text{(SRH proxy)} \\
V_{oc}^{loss} &= -\frac{E_g}{k_B T} + \log(R_{SRH}) & \text{($V_{oc}$ loss proxy)}
\end{align}

\subsubsection{I. Composite Physics Predictors (7 features)}
\begin{align}
\text{FF}_{pred} &= \log(\Theta_{min}) - \log(R_s^{total}) & \text{(Fill factor predictor)} \\
V_{oc}^{pred} &= E_g + 0.5(\Phi_{h,ETL} + \Phi_{e,HTL}) - k_B T \log(R_{SRH}) \\
J_{sc}^{pred} &= \log(J_{max}) + \log(\eta_{coll,min}) \\
\text{Quality} &= \log(\Theta_{min}) + \text{SRH}_{strength} - \log(R_s^{total})
\end{align}

\subsection{Analytical Ceiling Functions}

Physics-derived upper bounds used for ratio-based predictions:

\begin{align}
J_{sc}^{ceiling} &= q \cdot G_{avg} \cdot l_{Perov} \\
V_{oc}^{ceiling} &= \min(E_g, V_{bi}) \\
V_{mpp}^{est} &= V_{oc} - k_B T \cdot \ln(1 + V_{oc}/k_B T) & \text{(Shockley approx.)} \\
\text{FF}^{est} &= \frac{v_{oc} - \ln(v_{oc} + 0.72)}{v_{oc} + 1}, \quad v_{oc} = \frac{V_{oc}}{k_B T} & \text{(Green formula)}
\end{align}

%=============================================================================
\section{Training \& Optimization}
%=============================================================================

\subsection{Primary Metric: $\Delta V$-Weighted Curve $R^2$}

\subsubsection{$\Delta V$ Weight Computation}
\begin{equation}
\Delta V_j = \begin{cases}
V_{j+1} - V_j & j < N_V \\
\Delta V_{N_V - 1} & j = N_V
\end{cases}
\end{equation}

For our grid: $\Delta V = 0.1$ V for $j \leq 4$, $\Delta V = 0.025$ V for $j > 4$.

\subsubsection{$\Delta V$-Weighted Curve Loss}
\begin{equation}
\mathcal{L}_{curve} = \frac{\sum_{j=1}^{45} \Delta V_j \cdot w_j \cdot \left(\hat{J}(V_j) - J(V_j)\right)^2}{\sum_{j=1}^{45} \Delta V_j \cdot w_j}
\end{equation}

where MPP weighting:
\begin{equation}
w_j = 1 + \alpha \cdot \exp\left(-\frac{(V_j - V_{mpp})^2}{2\sigma_{mpp}^2}\right), \quad \alpha = 2.0, \; \sigma_{mpp} = 0.1\text{V}
\end{equation}

\subsubsection{$\Delta V$-Weighted $R^2$ (Per-Curve)}
\begin{equation}
R^2_i = 1 - \frac{\sum_j \Delta V_j \left(\hat{J}_i(V_j) - J_i(V_j)\right)^2}{\sum_j \Delta V_j \left(J_i(V_j) - \bar{J}_i\right)^2}, \quad \bar{J}_i = \frac{\sum_j \Delta V_j \cdot J_i(V_j)}{\sum_j \Delta V_j}
\end{equation}

\textbf{Aggregates reported}: mean$(R^2_i)$, median$(R^2_i)$, $Q_{0.05}$, $Q_{0.95}$.

\subsubsection{Safe MAPE Variant}
\begin{equation}
\text{MAPE}_\epsilon = \frac{100\%}{N} \sum_i \left| \frac{\hat{y}_i - y_i}{\max(|y_i|, \epsilon)} \right|, \quad \epsilon = 0.1 \text{ mA/cm}^2
\end{equation}

\subsection{Multi-Task Loss (Kendall Uncertainty)}

\begin{equation}
\mathcal{L}_{multi} = \sum_{t=1}^{4} \frac{1}{2\sigma_t^2} \mathcal{L}_t + \log \sigma_t
\end{equation}

Tasks: $\mathcal{L}_{J_{sc}}$, $\mathcal{L}_{V_{oc}}$, $\mathcal{L}_{V_{mpp}}$, $\mathcal{L}_{J_{mpp}}$ \cite{kendall2018multi}.

\subsection{Continuity Loss (Grid-Aware)}

\begin{equation}
\mathcal{L}_{cont} = \left| \frac{J(V_{mpp}) - J(V_{mpp}^-)}{\Delta V^-} - \frac{J(V_{mpp}^+) - J(V_{mpp})}{\Delta V^+} \right|^2
\end{equation}

\subsection{Total Loss}
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{curve} + \lambda_{multi} \mathcal{L}_{multi} + \lambda_{cont} \mathcal{L}_{cont} + \lambda_{jac} \|\nabla_x f\|_F^2
\end{equation}

Defaults: $\lambda_{multi} = 1.0$, $\lambda_{cont} = 0.1$, $\lambda_{jac} = 0$.

\subsection{Optimization Configuration}

\begin{table}[h]
\centering
\caption{Training Configuration}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & AdamW & $\beta = (0.9, 0.999)$ \\
Learning rate & HPO range & [$10^{-4}$, $2 \times 10^{-3}$] \\
Weight decay & HPO range & [$10^{-6}$, $10^{-4}$] \\
Scheduler & ReduceLROnPlateau & factor=0.5, patience=10 \\
Early stopping & patience & 30 epochs \\
Gradient clipping & max\_norm & 1.0 \\
Mixed precision & AMP & Enabled \\
Max epochs & & 300 \\
Batch size & & 256 (HPO: 128-1024) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{NaN/Inf Protection}

Critical safeguards during training:
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
# Jacobian guard
if torch.isnan(jac_norm) or torch.isinf(jac_norm):
    jac_norm = torch.tensor(0.0, device=device)
jac_norm = jac_norm.clamp(max=100.0)

# Loss guard
if torch.isnan(loss) or torch.isinf(loss):
    continue  # Skip batch
\end{lstlisting}

%=============================================================================
\section{Baseline Models}
%=============================================================================

\subsection{CVAE Baseline (Conditional Variational Autoencoder)}

\begin{table}[h]
\centering
\caption{CVAE Architecture}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Architecture} & \textbf{Notes} \\
\midrule
Encoder input & curve (45) + condition (102) = 147 & \\
Encoder & 147 $\rightarrow$ 256 $\rightarrow$ 128 & SiLU activation \\
Latent & $\mu$, $\log\sigma \in \mathbb{R}^{16}$ & latent\_dim=16 \\
Decoder input & latent (16) + condition (102) = 118 & \\
Decoder & 118 $\rightarrow$ 128 $\rightarrow$ 256 $\rightarrow$ 45 & SiLU activation \\
\bottomrule
\end{tabular}
\end{table}

\textbf{CVAE Loss}:
\begin{equation}
\mathcal{L}_{CVAE} = \text{MSE}(\hat{J}, J) + \beta \cdot D_{KL}(q(z|J,x) \| p(z))
\end{equation}

Default $\beta = 0.001$ (small to prioritize reconstruction).

\subsection{DirectCurveShapeNet (Alternative Main Model)}

Decouples endpoint prediction from shape:
\begin{enumerate}
    \item \textbf{Pretrained $J_{sc}$}: From LGBM ($R^2 \approx 0.965$)
    \item \textbf{Pretrained $V_{oc}$}: From VocNN ($R^2 \approx 0.73$)
    \item \textbf{Shape network}: Predicts normalized curve $\tilde{J}(V) = 2 \cdot J(V)/J_{sc} - 1$
\end{enumerate}

\subsection{Direct MLP Baseline}

Simple feedforward: $31 \rightarrow 256 \rightarrow 128 \rightarrow 45$ (no physics structure).

%=============================================================================
\section{Dataset \& Preprocessing}
%=============================================================================

\subsection{Dataset Statistics}

\begin{itemize}
    \item \textbf{Primary}: 100k samples (LHS\_parameters\_m.txt, IV\_m.txt)
    \item \textbf{Augmented}: 300k samples (LHS\_parameters\_m\_300k.txt, IV\_m\_300k.txt)
    \item \textbf{Split}: 70/15/15 train/val/test (stratified by $V_{oc}$ bins)
\end{itemize}

\subsection{Preprocessing Pipeline}

\begin{enumerate}
    \item \textbf{Log transform}: $x' = \log_{10}(x + 1)$ for mobilities, lifetimes, doping
    \item \textbf{RobustScaler}: $x'' = (x' - \text{median}) / \text{IQR}$
    \item \textbf{MinMaxScaler}: $x''' \in [0, 1]$
\end{enumerate}

\subsection{Curve Preprocessing}

\begin{enumerate}
    \item \textbf{Isc normalization}: $\tilde{J}(V) = 2 \cdot J(V)/J_{sc} - 1 \in [-1, 1]$
    \item \textbf{Voc clamping}: $J(V > V_{oc}) = 0$
\end{enumerate}

\subsection{Train-Only Feature Selection}

\textbf{Step 1}: Remove if $\max_y |r(x_i, y)| < 0.3$ for all targets.

\textbf{Step 2}: For pairs with $|r(x_i, x_j)| > 0.9$, remove the one with lower target correlation.

\textbf{Output}: Feature mask + train split hash for reproducibility.

%=============================================================================
\section{Evaluation Metrics}
%=============================================================================

\begin{table}[h]
\centering
\caption{Complete Metric Suite}
\begin{tabular}{llll}
\toprule
\textbf{Metric} & \textbf{Formula} & \textbf{Role} & \textbf{Target} \\
\midrule
\multicolumn{4}{l}{\textit{Primary (Model Selection)}} \\
Mean $R^2_{curve}$ & $\frac{1}{N}\sum_i R^2_i$ ($\Delta V$-weighted) & Selection & $> 0.99$ \\
\midrule
\multicolumn{4}{l}{\textit{Secondary (Reporting)}} \\
Median $R^2_{curve}$ & median$(R^2_i)$ & Robustness & $> 0.995$ \\
5th pct $R^2_{curve}$ & $Q_{0.05}(R^2_i)$ & Worst-case & $> 0.95$ \\
MAPE$_\epsilon$ (curve) & Clipped MAPE & Relative error & $< 5\%$ \\
MAPE$_\epsilon$ (FF) & On fill factor & Device metric & $< 2\%$ \\
MAE (anchors) & mA/cm$^2$ or V & Absolute & varies \\
\midrule
\multicolumn{4}{l}{\textit{Constraint Metrics}} \\
Pre-proj violations & Count / 1000 & Learning & $\rightarrow 0$ \\
Post-recon monotonicity & Count / 1000 & Guarantee & $= 0$ \\
Inference time & ms/sample & Efficiency & $< 1$ ms \\
\bottomrule
\end{tabular}
\end{table}

%=============================================================================
\section{Ablation Study Design}
%=============================================================================

\subsection{Tier 0: Must-Have Ablations (Main Paper)}

\begin{table}[h]
\centering
\caption{Tier 0 Ablation Matrix (3 seeds each)}
\begin{tabular}{llp{6cm}}
\toprule
\textbf{ID} & \textbf{Configuration} & \textbf{Tests Claim} \\
\midrule
T0-1 & Main model (full) & Baseline performance \\
T0-2 & No split (single spline) & Why split at MPP matters \\
T0-3 & No anchors (direct 45-out) & Why anchoring helps FF \\
T0-4 & No projection & Hard constraints prevent violations \\
T0-5 & No physics features (raw 31) & Value of feature engineering \\
T0-6 & CVAE baseline & Structured vs generative \\
T0-7 & Direct MLP baseline & vs end-to-end black box \\
\midrule
\multicolumn{3}{l}{\textbf{Total: 7 configs $\times$ 3 seeds = 21 runs}} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Tier 1: Strongly Recommended (Appendix)}

\begin{table}[h]
\centering
\caption{Tier 1 Sweeps (3 seeds each)}
\begin{tabular}{lll}
\toprule
\textbf{Sweep} & \textbf{Values} & \textbf{Runs} \\
\midrule
Control points $K$ & \{2, 4, 6, 8\} & 12 \\
Continuity $\lambda_{cont}$ & \{0, 0.01, 0.1, 0.5, 1.0\} & 15 \\
Feature count $m$ & \{0, 2, 4, 8, 16, 71\} & 18 \\
\midrule
\textbf{Total} & & 45 runs \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Tier 2: Nice-to-Have (If Time)}

\begin{itemize}
    \item OOD splits by bandgap regime, thickness regime
    \item Uncertainty calibration (MC-Dropout, ensemble)
    \item Jacobian weight ablation: $\lambda_{jac} \in \{0, 10^{-5}, 10^{-4}, 10^{-3}\}$
\end{itemize}

\subsection{Total Experiment Budget}

\begin{table}[h]
\centering
\caption{Experiment Budget Summary}
\begin{tabular}{lcc}
\toprule
\textbf{Tier} & \textbf{Runs} & \textbf{Compute Est.} \\
\midrule
Tier 0 (must-have) & 21 & $\sim$2 days (24 GPU-hours) \\
Tier 1 (recommended) & 45 & $\sim$4 days (48 GPU-hours) \\
Tier 2 (optional) & variable & as time permits \\
\midrule
\textbf{Total minimum} & 66 & $\sim$6 days \\
\bottomrule
\end{tabular}
\end{table}

%=============================================================================
\section{Figures to Produce}
%=============================================================================

\subsection{Main Paper Figures (5)}

\begin{enumerate}
    \item \textbf{Method schematic}: Input $\rightarrow$ feature selection $\rightarrow$ backbone $\rightarrow$ heads $\rightarrow$ $\Pi_\mathcal{C}$ $\rightarrow$ split PCHIP $\rightarrow$ curve. Show split at MPP.

    \item \textbf{$R^2$ distribution (violin)}: Main model vs baselines (Direct MLP, CVAE, no-split). Shows tail behavior.

    \item \textbf{J-V overlays (3$\times$3 grid)}: 9 curves, true vs predicted, markers at 45 grid points, anchors marked.

    \item \textbf{Ablation heatmap}: Rows = T0-1 through T0-7. Columns = mean $R^2$, MAPE, violations/1000, ms.

    \item \textbf{Violation learning curve}: Pre-projection violation rate vs epoch (log y-axis). The ``physics learning'' money plot.
\end{enumerate}

\subsection{Main Paper Tables (2)}

\begin{enumerate}
    \item \textbf{Table 1}: Baselines comparison (mean$\pm$std over 3 seeds): $R^2$, MAPE, FF MAPE, violations, inference time.

    \item \textbf{Table 2}: Key ablations (same metrics).
\end{enumerate}

\subsection{Appendix Figures}

\begin{enumerate}
    \item Non-uniform voltage grid visualization with $\Delta V$ annotations
    \item Feature selection stability (frequency bar chart across 10 seeds)
    \item $K$ sweep: $R^2$ vs control points
    \item $\lambda_{cont}$ sweep: $R^2$ vs continuity weight
    \item Metric robustness: unweighted vs $\Delta V$-weighted evaluation
    \item Error concentration: weighted squared error vs $V/V_{oc}$
    \item CVAE $\beta$ sweep
    \item Feature importance (SHAP or permutation)
\end{enumerate}

%=============================================================================
\section{Code Implementation Checklist}
%=============================================================================

\subsection{Must Implement / Verify}

\begin{enumerate}
    \item \texttt{metrics\_curve.py}
    \begin{itemize}
        \item \texttt{deltaV(V\_grid)} $\rightarrow$ length-45 weights
        \item \texttt{curve\_sse\_weighted(Jhat, J, V, w=None)}
        \item \texttt{curve\_r2\_weighted(Jhat, J, V, w=None)} $\rightarrow$ per-curve $R^2_i$ + aggregates
        \item \texttt{curve\_mape\_safe(Jhat, J, eps=0.1)}
    \end{itemize}

    \item \texttt{run\_experiments.py}
    \begin{itemize}
        \item Reads \texttt{run\_manifest.yaml}
        \item Launches jobs (local or SLURM)
        \item Writes to \texttt{outputs/<exp\_id>/<seed>/}
        \item Merges to \texttt{results.csv}
    \end{itemize}

    \item \texttt{plot\_paper\_figs.py}
    \begin{itemize}
        \item \texttt{fig\_r2\_distribution.pdf}
        \item \texttt{fig\_jv\_overlays.pdf}
        \item \texttt{fig\_ablation\_heatmap.pdf}
        \item \texttt{fig\_violation\_curve.pdf}
    \end{itemize}

    \item Baseline adapters (same 45-point grid output):
    \begin{itemize}
        \item \texttt{baseline\_direct\_mlp.py}
        \item \texttt{baseline\_cvae.py} (already exists, verify grid)
    \end{itemize}
\end{enumerate}

\subsection{Artifacts to Produce}

\texttt{results.csv} columns:
\begin{itemize}
    \item exp\_id, seed, split\_id
    \item mean\_r2, median\_r2, p5\_r2, p95\_r2
    \item mape\_safe, ff\_mape
    \item violations\_per\_1000, inference\_ms
    \item selected\_feature\_names (JSON), m (feature count)
\end{itemize}

\subsection{Quality Checks (Fail-Fast Assertions)}

\begin{itemize}
    \item Feature selection fit only on train indices (log hash)
    \item Knots strictly increasing (voltage), non-increasing (current)
    \item Reconstructed curve monotone decreasing
    \item Evaluation uses $\Delta V$-weighted metrics
    \item No ground truth leakage in projection
\end{itemize}

%=============================================================================
\section{Missing Elements \& Acquisition Plan}
%=============================================================================

\subsection{High Priority}

\begin{table}[h]
\centering
\caption{High Priority Tasks}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Effort} & \textbf{Compute} & \textbf{Status} \\
\midrule
Run Tier 0 ablations & Low & 21 runs & TODO \\
Generate 5 main figures & Medium & - & TODO \\
Generate 2 main tables & Low & - & TODO \\
Feature selection stability & Low & 10 runs & TODO \\
Implement metrics\_curve.py & Low & - & TODO \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Medium Priority}

\begin{table}[h]
\centering
\caption{Medium Priority Tasks}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Effort} & \textbf{Compute} & \textbf{Status} \\
\midrule
Run Tier 1 sweeps & Low & 45 runs & TODO \\
Generate appendix figures & Medium & - & TODO \\
Deep ensemble (5 models) & Medium & 5$\times$ & TODO \\
SHAP analysis & Low & minutes & TODO \\
Input perturbation test & Low & minutes & TODO \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Lower Priority}

\begin{itemize}
    \item OOD detection / leave-one-out material
    \item Sobol sensitivity indices
    \item Conformal prediction intervals
    \item Jacobian regularization ablation
\end{itemize}

%=============================================================================
\section{Conclusion}
%=============================================================================

This document provides comprehensive technical documentation for the physics-constrained split-spline framework. Key contributions:

\begin{enumerate}
    \item \textbf{UnifiedSplitSplineNet}: Multi-head architecture with joint anchor and control point prediction
    \item \textbf{Hard physics projection} $\Pi_\mathcal{C}$: Differentiable, guarantees physical validity
    \item \textbf{PCHIP reconstruction}: Monotonicity-preserving via Fritsch-Carlson slopes
    \item \textbf{$\Delta V$-weighted metrics}: Proper evaluation for non-uniform voltage grid
    \item \textbf{71 physics features}: Comprehensive drift-diffusion-derived feature engineering
    \item \textbf{Train-only selection}: Leak-free reduction to $m \approx 4$ features
\end{enumerate}

The experiment plan (66 runs minimum) and figure list provide a clear roadmap for ICML submission.

\appendix

\section{References}

\begin{thebibliography}{9}

\bibitem{fritsch1980monotone}
F.N. Fritsch and R.E. Carlson,
``Monotone Piecewise Cubic Interpolation,''
\textit{SIAM J. Numer. Anal.}, 17(2), 238-246, 1980.

\bibitem{kendall2018multi}
A. Kendall, Y. Gal, and R. Cipolla,
``Multi-Task Learning Using Uncertainty to Weigh Losses,''
\textit{CVPR}, 2018.

\end{thebibliography}

\section{Physical Constants}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Constant} & \textbf{Symbol} & \textbf{Value} \\
\midrule
Boltzmann $\times$ Temperature & $k_B T$ & 0.0259 eV (300K) \\
Elementary charge & $q$ & $1.602 \times 10^{-19}$ C \\
\bottomrule
\end{tabular}
\end{table}

\section{Voltage Grid Values}

The exact 45-point non-uniform grid:
\begin{lstlisting}[basicstyle=\small\ttfamily]
[0.000, 0.100, 0.200, 0.300, 0.400,  # 5 points, dV=0.1
 0.425, 0.450, 0.475, 0.500, 0.525,  # 40 points, dV=0.025
 0.550, 0.575, 0.600, 0.625, 0.650,
 0.675, 0.700, 0.725, 0.750, 0.775,
 0.800, 0.825, 0.850, 0.875, 0.900,
 0.925, 0.950, 0.975, 1.000, 1.025,
 1.050, 1.075, 1.100, 1.125, 1.150,
 1.175, 1.200, 1.225, 1.250, 1.275,
 1.300, 1.325, 1.350, 1.375, 1.400]
\end{lstlisting}

\end{document}

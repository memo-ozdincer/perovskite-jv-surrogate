PHYSICS-CONSTRAINED SPLIT-SPLINE J-V CURVE RECONSTRUCTION FOR PEROVSKITE SOLAR CELLS
CONDENSED IMPLEMENTATION PLAN (AS-BUILT PIPELINE v4.0 - DIRECT CURVE V2 EDITION)

================================================================================
PROBLEM STATEMENT
================================================================================
Objective: Predict full J-V curves (45 voltage-current pairs) for perovskite solar cells from 31 material parameters with >99.9% accuracy. Challenge: Direct MLP prediction ignores physical boundaries (J cannot exceed Jsc, V cannot exceed Voc) and produces non-monotonic artifacts at the maximum power point (MPP) "knee". Solution: Multi-head neural network that explicitly predicts physical anchors (Jsc, Voc, Vmpp, Jmpp) with hard constraint projection, then reconstructs curve shape using split PCHIP interpolation at MPP to decouple transport-limited (Region 1: 0→Vmpp) from recombination-limited physics (Region 2: Vmpp→Voc).

================================================================================
DATA & PHYSICS CONTEXT
================================================================================
Files: `data.py` handles loading, `config.py` defines grid/constants.
Input: 31 COMSOL-simulated material parameters (layer thicknesses, mobilities, carrier lifetimes, doping, work functions, recombination coefficients).
Curves: 45-point J-V characteristics on non-uniform voltage grid [0, 0.1, 0.2, 0.3, 0.4, 0.425, 0.45, ..., 1.4V] totaling 45 evaluation points (`V_GRID` in `config.py`).
Physics features: 71 engineered features computed in `features.py:compute_all_physics_features` from raw params. Includes debye_length, diffusion_length, mobility_ratios, dos_ratios, energy_differences, generation_rate, recombination_timescales, field_strengths, einstein_relations, ideality_factor_proxies, plus thermodynamic ceilings (jsc_ceiling from q×G×L, voc_ceiling from Shockley-Queisser).

Feature validation: Two-stage filtering implemented:
  1. Pearson correlation analysis (`features.py:validate_physics_features`) drops weak features (|r|<0.3 vs targets)
  2. Multicollinearity check (`logging_utils.py:compute_multicollinearity`) identifies highly correlated feature pairs (|r|>0.85)

Masking logic: `train.py:ScalarPredictorPipeline._apply_feature_mask` applies both filters.

================================================================================
IMPLEMENTED ARCHITECTURE (AS-BUILT)
================================================================================
File: `models/voc_nn.py`

Primary curve model (current): ControlPointNet (control-point-only).
  Inputs:
    - Features: 31 raw params + 71 physics features (derived only from the 31 params)
    - Anchors: [Jsc, Voc, Vmpp, Jmpp] predicted by upstream NN/LGBM models
  Output:
    - Region 1 control points (0→Vmpp)
    - Region 2 control points (Vmpp→Voc)
  Reconstruction:
    - `reconstruct_curve` uses the predicted anchors + control points

Legacy model (kept for compatibility): UnifiedSplitSplineNet.
  Input: 102 features (31 raw + 71 physics, normalized independently)
  Output: Anchors + control points in one network
  NOTE: Not the default curve model.

CONFIGURABLE CONTROL POINTS (v2.0):
  Default K=4 (reduced from 6 for simpler models)
  Set via --ctrl-points CLI argument
  Each region has K+2 knots total (2 anchors + K interior points)

================================================================================
HARD CONSTRAINT PROJECTION (WITH VIOLATION LOGGING)
================================================================================
File: `models/voc_nn.py` function `physics_projection`

Implemented: Differentiable clamping via torch.clamp ensuring:
  Jsc > 1e-6 (strict positivity)
  Voc > 1e-6 (strict positivity)
  1e-6 < Vmpp < Voc - 1e-6 (strict ordering)
  1e-6 < Jmpp < Jsc - 1e-6 (strict ordering)

NEW IN v2.0 - Violation Logging:
  physics_projection(..., return_violations=True) returns violation counts BEFORE clamping:
    - jsc_negative: count of Jsc < 1e-6
    - voc_negative: count of Voc < 1e-6
    - vmpp_exceeds_voc: count of Vmpp >= Voc
    - jmpp_exceeds_jsc: count of Jmpp >= Jsc

  This diagnoses how often the network "wants" to violate constraints during training.
  Logged per-epoch to constraint_violations.csv via TrainingLogger.

================================================================================
SPLIT-SPLINE RECONSTRUCTION LAYER (DIFFERENTIABLE PCHIP)
================================================================================
File: `models/reconstruction.py`

build_knots() constructs monotonic voltage-current knot sequences:
  CUMULATIVE SCALING (guarantees monotonicity):
    - ctrl values are sigmoid outputs in [0,1]
    - cumsum gives strictly increasing sequence
    - Normalizing by final cumsum ensures exact endpoint matching
  - j_interior interpolates between Jsc→Jmpp (Region 1) or Jmpp→J_end (Region 2)

  Region 1: K+2 knots from (V=0, J=Jsc) through K interior points to (V=Vmpp, J=Jmpp)
  Region 2: K+2 knots from (V=Vmpp, J=Jmpp) through K interior points to (V=Voc, J=J_end)

NEW IN v2.0 - Monotonicity Violation Tracking:
  build_knots(..., return_violation_counts=True) returns:
    - region1_violations: samples with non-monotonic J in Region 1
    - region2_violations: samples with non-monotonic J in Region 2
    - total_samples: batch size

  Logged to monotonicity.csv for debugging spline construction issues.

Interpolation: `pchip_interpolate_batch` implements Fritsch-Carlson PCHIP in PyTorch.
Stitching: Curves split at Vmpp, concatenated with mask-based selection.
Endpoint enforcement (new):
  - After PCHIP, set J(V=0)=Jsc, set J(Vmpp)=Jmpp, and clamp J to J_end at V>=Voc
  - Normalized curves use J_end=-1 (Isc-normalized space)
  - Absolute curves use J_end=0
Tail handling: L_tail = relu(J[v>Voc]).mean() + hard clamp at inference.

================================================================================
LOSS FUNCTION (KENDALL MULTI-TASK + CONTINUITY)
================================================================================
File: `train.py` class `MultiTaskLoss`

Kendall multi-task learning with learnable log-variances:
  Parameters: log_sigma_anchor, log_sigma_curve (trainable)
  Formula: L = L_anchor/(2σ_a²) + log(σ_a) + L_curve/(2σ_c²) + log(σ_c)

  L_anchor = MSE(pred_anchors, true_anchors) on 4 scalars
  L_curve = MSE(reconstructed_curve, true_curve) on 45 points

NEW IN v2.0 - Sigma Logging:
  MultiTaskLoss tracks:
    - sigma_anchor, sigma_curve: actual σ values (exp of log_sigma)
    - sigma_ratio: σ_anchor / σ_curve (detect task imbalance)
    - task_imbalance: True if ratio > 10 or < 0.1

  Logged per-epoch to multitask_losses.csv.
  Console warnings when task imbalance detected.

Continuity regularization: `models/reconstruction.py:continuity_loss`
  L_cont = (J1(Vmpp) - J2(Vmpp))² + (dJ1/dV - dJ2/dV)²
  Weight configurable via --continuity-weight (default 0.5, try 0.1-1.0)

Total loss: L_total = L_weighted + λ_cont × L_cont

================================================================================
TRAIN-TEST MISMATCH FIX (CRITICAL v2.0 CHANGE)
================================================================================
PROBLEM IDENTIFIED:
  Original: Soft penalty during training, hard clamp only at inference
  Result: Network never learns boundary behavior, performance drops at test time

SOLUTION IMPLEMENTED:
  Hard clamping (torch.clamp) now applied DURING TRAINING by default
  Network learns correct behavior at constraint boundaries
  No train-test distribution shift

CLI Control:
  Default: Hard clamp during training (use_hard_clamp_training=True)
  Override: --soft-clamp-training to use soft penalties (not recommended)

================================================================================
MULTICOLLINEARITY DETECTION (NEW IN v2.0)
================================================================================
File: `logging_utils.py`

Functions:
  compute_multicollinearity(features, threshold=0.85):
    Returns correlation matrix and list of highly correlated pairs

  suggest_features_to_drop(corr_matrix, feature_names, target_correlations, threshold):
    When two features are highly correlated, keeps the one with higher target correlation

Integration in train.py:
  ScalarPredictorPipeline.run_feature_validation() computes multicollinearity
  If --drop-multicollinear flag set, removes redundant features
  Report saved to multicollinearity.json

Threshold configurable via --multicollinearity-threshold (default 0.85)

================================================================================
LOGGING INFRASTRUCTURE (NEW IN v2.0)
================================================================================
File: `logging_utils.py`

TrainingLogger class provides centralized logging:

Dataclasses for structured logging:
  ConstraintViolationLog: epoch, batch, jsc_negative, voc_negative, vmpp_exceeds_voc, jmpp_exceeds_jsc, total_samples
  MultiTaskLossLog: epoch, loss_anchor, loss_curve, sigma_anchor, sigma_curve, loss_continuity, loss_tail, loss_total
  MonotonicityLog: epoch, batch, region1_violations, region2_violations, total_samples
  ModelComparisonMetrics: model_name, mse_full_curve, mse_region1/2, mae_jsc/voc/vmpp/jmpp, mape_ff, violations_*, inference_time_ms

Output files generated:
  constraint_violations.csv   - Per-epoch constraint violation attempts
  multitask_losses.csv        - Per-epoch sigma values and loss components
  monotonicity.csv            - Spline knot monotonicity violations
  multicollinearity.json      - Feature correlation analysis
  model_comparison.json       - Split-Spline vs CVAE metrics
  model_comparison.md         - Markdown comparison table
  training_summary.json       - Overall training summary with duration, final sigmas, violation rates

TrainingLogger.generate_comparison_table() produces formatted markdown:
  | Model | MSE Full | MSE R1 | MSE R2 | FF MAPE (%) | Violations/1000 | Time (ms) |

================================================================================
HYPERPARAMETER OPTIMIZATION (SIMPLIFIED IN v2.0)
================================================================================
File: `hpo.py`

SIMPLIFIED SEARCH SPACES (narrower ranges for faster convergence):

VOC NN (sample_voc_nn_config):
  - Layers: 2-3 (was 2-4)
  - Hidden dim: 256-768 (unchanged)
  - Learning rate: 5e-5 to 1e-3 (narrower, was 1e-5 to 1e-2)
  - Normalization: LayerNorm always (removed BatchNorm option)
  - Activation: gelu or silu only (removed relu, swish, mish)
  - Dropout: 0.1-0.3 (unchanged)
  - Weight decay: 1e-5 to 1e-2 (unchanged)

LightGBM (sample_lgbm_config):
  - Boosting: GBDT only (removed DART - too slow)
  - Learning rate: 0.01-0.1 (narrower, was 0.005-0.2)
  - Num leaves: 31-255 (unchanged)
  - Max depth: 6-12 (unchanged)
  - Feature/bagging fraction: 0.7-1.0 (unchanged)
  - Min child samples: 5-50 (unchanged)

HPO Trials (configurable):
  --hpo-trials-nn: Default 50 (was 100)
  --hpo-trials-lgbm: Default 100 (was 200)
  --hpo-timeout: Default 7200 seconds (2 hours per model)

FIX: Added Jmpp LGBM to run_full_hpo() (was missing)
NEW: Added Voc LGBM HPO with ceiling ratio target

================================================================================
CVAE BASELINE (WITH COMPARISON LOGGING)
================================================================================
File: `models/cvae.py`

Conditional Variational Autoencoder for comparison only:
  Encoder: curve + params → latent_dim=16 via mu/logvar
  Decoder: latent + params → reconstructed curve
  Loss: cvae_loss = MSE_reconstruction + β×KL_divergence (β=0.001)

NEW IN v2.0 - Comparison Logging:
  evaluate_cvae() now creates ModelComparisonMetrics:
    - mse_full_curve, mse_region1, mse_region2
    - mae_jsc, mae_voc, mae_vmpp, mae_jmpp
    - mape_ff
    - violations_jsc_negative, violations_voc_negative, etc.
    - violations_j_exceeds_jsc (J > Jsc anywhere on curve)
    - inference_time_ms

  Logged alongside Split-Spline metrics for side-by-side comparison.
  The production curve model is ControlPointNet, not CVAE.

================================================================================
TRAINING PIPELINE
================================================================================
File: `train.py` class `ScalarPredictorPipeline`

Constructor options (updated for v2.0):
  - output_dir: Where to save models and logs
  - device: 'cuda' or 'cpu'
  - drop_weak_features: Remove features with |r| < 0.3
  - multicollinearity_threshold: Default 0.85
  - drop_multicollinear: Remove redundant features
  - continuity_weight: Default 0.5 (try 0.1-1.0)
  - ctrl_points: Default 4 (try 4-6)
  - use_hard_clamp_training: Default True (fixes train-test mismatch)
  - log_constraint_violations: Default True
  - verbose_logging: Default True

Training flow:
  1. Load data, compute physics features
  2. Run feature validation (correlation + multicollinearity)
  3. Train Voc NN + Voc LGBM, select best on val
  4. Train Jsc LGBM
  5. Train Vmpp/Jmpp/FF LGBMs using predicted intermediates (no oracle inputs)
  6. Train ControlPointNet for curve reconstruction using predicted anchors
  7. Optionally train CVAE baseline (comparison only)
  8. Evaluate all models using predicted-chain inputs
  9. Save all logs and checkpoints

train_curve_model() loop:
  - Hard clamping during training (if use_hard_clamp_training=True)
  - Log sigma values and constraint violations per epoch
  - Track monotonicity violations from build_knots
  - Configurable continuity weight

================================================================================
CLI ARGUMENTS (UPDATED FOR v2.0)
================================================================================
python train.py [options]

Data options:
  --params FILE          LHS parameters file
  --iv FILE              IV curves file
  --output DIR           Output directory

Feature options:
  --drop-weak-features          Drop features with weak target correlation
  --drop-multicollinear         Drop multicollinear features
  --multicollinearity-threshold FLOAT  Threshold for multicollinearity (default 0.85)

Training options:
  --train-curves         Train split-spline curve model
  --train-cvae           Train CVAE baseline for comparison
  --direct-curve         Use V1 direct curve (control points + PCHIP)
  --direct-curve-v2      Use V2 direct curve (high-accuracy, 45-point direct) [NEW v4.0]
  --device DEVICE        cuda or cpu (default: cuda)

Model options:
  --ctrl-points INT      Number of control points per region (default 4)
  --continuity-weight FLOAT  Weight for continuity loss (default 0.5)
  --soft-clamp-training  Use soft penalties instead of hard clamp (not recommended)

HPO options:
  --hpo-trials-nn INT    HPO trials for neural networks (default 50)
  --hpo-trials-lgbm INT  HPO trials for LightGBM (default 100)
  --hpo-timeout INT      HPO timeout in seconds (default 7200)

Logging options:
  --no-constraint-logging  Disable constraint violation logging
  --quiet                  Reduce console output

================================================================================
OUTPUT FILES GENERATED
================================================================================
After training with --train-curves --train-cvae:

Models:
  models/ctrl_point_model.pt     - ControlPointNet checkpoint (V1 curve model)
  models/direct_curve_v2_model.pt - DirectCurveNetV2 checkpoint (V2 high-accuracy) [NEW v4.0]
  models/curve_model.pt          - Legacy UnifiedSplitSplineNet checkpoint (if trained)
  models/cvae_model.pt           - CVAE checkpoint (comparison only)
  models/voc_nn.pt               - Voc neural network
  models/voc_lgbm.txt            - Voc LightGBM
  models/jsc_lgbm.txt            - Jsc LightGBM
  models/vmpp_lgbm.txt           - Vmpp LightGBM
  models/jmpp_lgbm.txt           - Jmpp LightGBM
  models/ff_lgbm.txt             - FF LightGBM
  models/preprocessor.joblib     - Parameter/curve preprocessing state

Logs (new in v2.0):
  constraint_violations.csv   - How often network tried to violate physics
  multitask_losses.csv        - Sigma values per epoch (detect task imbalance)
  monotonicity.csv            - Spline knot monotonicity violations
  multicollinearity.json      - Feature correlation analysis
  model_comparison.json       - Split-Spline vs CVAE raw metrics
  model_comparison.md         - Formatted comparison table
  training_summary.json       - Duration, final sigmas, overall violation rate

Metrics:
  metrics.json                - Final test set metrics

================================================================================
SLURM SCRIPTS (UPDATED FOR v4.0)
================================================================================

AVAILABLE SCRIPTS:

1. slurm_curve_pipeline.sh - Original split-spline pipeline
   Time: 30 hours, Full HPO + CVAE comparison

2. slurm_direct_hpo.sh - V1 direct curve WITH HPO
   Time: 20 hours, Control points + PCHIP

3. slurm_direct_no_hpo.sh - V1 direct curve WITHOUT HPO (fast)
   Time: 4 hours, Default hyperparameters

4. slurm_direct_v2.sh - V2 direct curve (HIGH-ACCURACY) [NEW v4.0]
   Time: 12 hours, Direct 45-point prediction + parameter reconstruction
   TARGET: >99% R² following Zbinden (2026) and Toprak (2025)

--------------------------------------------------------------------------------
V1 SPLIT-SPLINE: slurm_curve_pipeline.sh
--------------------------------------------------------------------------------
Configuration variables:
  HPO_TRIALS_NN=50            # Reduced due to simplified search space
  HPO_TRIALS_LGBM=100         # Reduced due to simplified search space
  HPO_TIMEOUT=7200            # 2 hours per model
  CONTINUITY_WEIGHT=0.5       # Try 0.1, 0.5, or 1.0
  CTRL_POINTS=4               # Simplified from 6

Time limit: 30 hours (for unattended overnight runs)

Full command:
  python train.py \
    --params "$WORK_DIR/LHS_parameters_m.txt" \
    --iv "$WORK_DIR/IV_m.txt" \
    --output "$OUT_DIR" \
    --device cuda \
    --train-curves \
    --train-cvae \
    --drop-weak-features \
    --drop-multicollinear \
    --hpo-trials-nn $HPO_TRIALS_NN \
    --hpo-trials-lgbm $HPO_TRIALS_LGBM \
    --hpo-timeout $HPO_TIMEOUT \
    --continuity-weight $CONTINUITY_WEIGHT \
    --ctrl-points $CTRL_POINTS

--------------------------------------------------------------------------------
V2 HIGH-ACCURACY: slurm_direct_v2.sh [NEW v4.0]
--------------------------------------------------------------------------------
Configuration variables:
  HPO_TRIALS_NN=5             # Only for Voc NN
  HPO_TRIALS_LGBM=100         # For Jsc, Vmpp, Jmpp
  HPO_TIMEOUT=7200            # 2 hours per model

Time limit: 12 hours

Features:
  - Direct 45-point prediction (no control point bottleneck)
  - Parameter reconstruction loss (6 key physics parameters)
  - Hard monotonicity via cumulative decrements
  - Region-weighted loss (3x weight on knee region)

Full command:
  python train.py \
    --params "$WORK_DIR/LHS_parameters_m.txt" "$WORK_DIR/LHS_parameters_m_300k.txt" \
    --iv "$WORK_DIR/IV_m.txt" "$WORK_DIR/IV_m_300k.txt" \
    --output "$OUT_DIR" \
    --device cuda \
    --train-curves \
    --direct-curve-v2 \
    --drop-weak-features \
    --drop-multicollinear \
    --hpo-trials-nn $HPO_TRIALS_NN \
    --hpo-trials-lgbm $HPO_TRIALS_LGBM \
    --hpo-timeout $HPO_TIMEOUT

To reuse HPO results from previous run:
  sbatch slurm_direct_v2.sh /path/to/hpo_results.json

================================================================================
INFERENCE PIPELINE
================================================================================
File: `inference.py`

Command: python inference.py --predict-curve --input params.csv --output curve_predictions.csv

Workflow:
  1. Load checkpoint (model weights, v_grid, feature_mask, normalization stats)
  2. Load preprocessor (param scaling + curve normalization)
  3. Compute 71 physics features from 31 input params
  4. Apply feature mask (weak features + multicollinear features dropped)
  5. Predict anchors using NN/LGBMs (Voc → Jsc → Vmpp → Jmpp)
  6. ControlPointNet predicts control points from (features + predicted anchors)
  7. Reconstruct curve with predicted anchors + control points (normalized PCHIP)
  8. Denormalize by predicted Jsc to absolute space

Uncertainty (--uncertainty flag): MC-Dropout with n_samples=100

================================================================================
DEBUGGING GUIDE (NEW IN v2.0)
================================================================================

If training stalls or performs poorly, check these logs:

1. SIGMA IMBALANCE (multitask_losses.csv):
   - If sigma_anchor << sigma_curve: Anchor task too easy, network ignoring curve
   - If sigma_curve << sigma_anchor: Curve task too easy, anchors dominating
   - Healthy: Both sigmas in similar range (0.3-1.0)

2. CONSTRAINT VIOLATIONS (constraint_violations.csv):
   - High jsc_negative or voc_negative: Network predicting negative values
   - High vmpp_exceeds_voc: Ordering constraint not learned
   - Should decrease over epochs as network learns

3. MONOTONICITY VIOLATIONS (monotonicity.csv):
   - Non-zero region1_violations or region2_violations: Spline construction issues
   - Should be zero with cumulative scaling

4. MULTICOLLINEARITY (multicollinearity.json):
   - Many high-correlation pairs: Redundant features causing instability
   - Use --drop-multicollinear to address

5. TASK IMBALANCE (console warnings):
   - "[!] Anchor task too easy" or "[!] Curve task too easy"
   - May need to adjust continuity weight or check data

================================================================================
VALIDATION CHECKLIST
================================================================================
After training, verify:
  [ ] constraint_violations.csv shows decreasing violation rate over epochs
  [ ] multitask_losses.csv shows balanced sigma values (ratio between 0.1-10)
  [ ] monotonicity.csv shows zero violations
  [ ] model_comparison.md shows Split-Spline beats CVAE on violations
  [ ] training_summary.json shows reasonable duration and final metrics
  [ ] metrics.json shows MSE < 0.001, FF MAPE < 5%

================================================================================
WHAT REMAINS (NEXT STEPS FOR PUBLICATION)
================================================================================
Ablation studies: No-split, no-continuity, no-anchors, no-physics-features, direct-MLP baselines.
OOD evaluation: Bandgap-based extrapolation, leave-one-material-out.
Uncertainty calibration: MC-Dropout intervals, calibration plots.
Experimental validation: NREL database curves, published J-V data.
Sensitivity analysis: Jacobians via autograd, physics sign validation.

================================================================================
KEY IMPLEMENTATION DETAILS FOR REPLICATION
================================================================================
Voltage grid: `config.py` hardcodes `V_GRID` (45 points)
Control points: Default K=4 per region → 6 knots total per region
Cumulative scaling: build_knots uses cumsum for guaranteed monotonicity
PCHIP slopes: Fritsch-Carlson with eps=1e-12 safe division
Continuity: Finite differences with local grid spacing
Kendall init: log_sigma = torch.zeros(1) → σ=1.0 initially
Feature mask: Boolean array in checkpoint, applied before physics computation
Hard clamp training: Default True, fixes train-test distribution shift
Logging: TrainingLogger accumulates logs, saves all on completion

================================================================================
DATA PREPROCESSING PIPELINE (v3.0 - KNOWN WORKING NORMALIZATION)
================================================================================
File: `preprocessing.py`

CRITICAL: train.py now uses Isc normalization via PVDataPreprocessor.

Curve Normalization by Isc:
  Formula: normalized = 2.0 * (curve / isc) - 1.0
  - Maps [0, isc] → [-1, 1] range
  - Decouples magnitude (Isc) from shape (easier learning)
  - First point: 2*(isc/isc) - 1 = 1.0
  - Last point (Voc): 2*(0/isc) - 1 = -1.0

  Implementation:
    def normalize_curves_by_isc(curves: np.ndarray) -> tuple:
        isc_values = curves[:, 0].copy()
        isc_safe = np.where(np.abs(isc_values) < 1e-9, 1.0, isc_values)
        normalized = 2.0 * (curves / isc_safe[:, np.newaxis]) - 1.0
        return isc_values, normalized

Denormalization (inverse):
  Formula: curve = (normalized + 1.0) / 2.0 * isc
  - Maps [-1, 1] → [0, isc]
  - MUST be applied before evaluation in absolute space
  - Round-trip: raw → normalize → denormalize → raw (error < 1e-6)

  Implementation:
    def denormalize_curves_by_isc(normalized, isc):
        is_tensor = isinstance(normalized, torch.Tensor)
        if is_tensor:
            isc = isc.unsqueeze(1) if isc.dim() == 1 else isc
        else:
            isc = isc[:, np.newaxis] if isc.ndim == 1 else isc
        return (normalized + 1.0) / 2.0 * isc

Parameter Transformations (RobustScaler + MinMaxScaler + log1p):
  WHY: Material properties span orders of magnitude (10^-5 to 10^27)
  - Simple Z-score fails due to outliers and heavy tails
  - RobustScaler: Uses median + IQR (robust to outliers)
  - MinMaxScaler: Maps to [-1, 1] (symmetric, matches NN activations)
  - log1p for material_properties: Compresses orders of magnitude

  Groups:
    layer_thickness: ['lH', 'lP', 'lE'] → RobustScaler → MinMaxScaler
    material_properties: [mobilities, DOS, energies, eps] → log1p → RobustScaler → MinMaxScaler
    contacts: ['Wlm', 'Whm'] → RobustScaler → MinMaxScaler
    recombination_gen: ['Gavg', 'Aug', 'Brad', lifetimes, velocities] → RobustScaler → MinMaxScaler

  Implementation:
    from sklearn.preprocessing import RobustScaler, MinMaxScaler, FunctionTransformer
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline

    def get_param_transformer(colnames):
        transformers = []
        for group, cols in param_defs.items():
            steps = [('robust', RobustScaler()), ('minmax', MinMaxScaler(feature_range=(-1, 1)))]
            if group == 'material_properties':
                steps.insert(0, ('log1p', FunctionTransformer(func=np.log1p)))
            transformers.append((group, Pipeline(steps), cols))
        return ColumnTransformer(transformers, remainder='passthrough')

PVDataPreprocessor class:
  Combines all preprocessing steps:
  - fit_transform_params(X_train) → fits transformers on training data
  - transform_params(X_val/test) → applies fitted transformers
  - fit_transform_curves(curves_train) → returns (isc, normalized_curves)
  - denormalize_curves(pred_norm, isc) → returns absolute predictions
  - save(filepath) / load(filepath) → for inference

Training Pipeline Changes (train.py):
  1. Fit preprocessor on training set:
       preprocessor = PVDataPreprocessor()
       X_train_transformed = preprocessor.fit_transform_params(X_train_full)
       isc_train, curves_train_norm = preprocessor.fit_transform_curves(curves_train)

  2. In training loop (normalized space):
       pred_curve_norm = reconstruct_curve_normalized(anchors_raw, ctrl1, ctrl2, v_grid)
       loss = mse_loss(pred_curve_norm, target_curves_norm)

  3. In evaluation (denormalize to absolute space):
       pred_curve_norm = reconstruct_curve_normalized(anchors, ctrl1, ctrl2, v_grid)
       pred_curve_abs = denormalize_curves_by_isc(pred_curve_norm, anchors[:, 0])
       mse = ((pred_curve_abs - true_curves_abs) ** 2).mean()

CRITICAL NOTES:
  - Curve reconstruction MUST use PREDICTED anchors (not raw inputs) ✓ Already correct
  - Loss can be computed in normalized space (better gradients)
  - Evaluation MUST be in absolute space for interpretability
  - Round-trip validation: normalize → denormalize should recover original

Validation Function:
  validate_curve_normalization(curves_raw, curves_norm, isc, n_samples=5)
  Checks:
    - Normalized range is ~[-1, 1]
    - Round-trip error < 1e-4
    - First point normalizes to 1.0
    - Last point normalizes to -1.0

Integration Status:
  ✅ preprocessing.py: Complete implementation
  ✅ train.py: Uses PVDataPreprocessor + Isc-normalized curves
  ✅ inference.py: Loads preprocessor + denormalizes curves

Expected Improvements:
  - 10-30% lower MSE (better-scaled gradients)
  - 30-50% faster convergence (normalized space easier to optimize)
  - Better generalization (RobustScaler handles outliers)
  - More stable training (no magnitude/shape coupling)

================================================================================
PLOTTING AND VISUALIZATION (PCHIP RECONSTRUCTION)
================================================================================
File: `train_updated.py` or add to `train.py`

ExamplePlotsCallback (for PyTorch Lightning):
  Purpose: Generate plots of reconstructed curves using PCHIP interpolation
  Features:
    - Plots fine-grid ground truth vs reconstructed predictions
    - Shows actual points (coarse grid) vs interpolated (fine grid)
    - Generates best/worst/random samples based on R² score
    - Logs to TensorBoard

  Implementation:
    class ExamplePlotsCallback(pl.Callback):
        def on_test_end(self, trainer, pl_module):
            # Get predictions and true curves
            preds, trues = pl_module.all_test_preds_np, pl_module.all_test_trues_np

            # Compute R² for ranking
            valid_mask = [i for i in range(len(trues)) if np.var(trues[i]) > 1e-6]
            metrics_df = pd.DataFrame({'r2': [r2_score(trues[i], preds[i]) for i in valid_mask]})

            # Select samples: random, best, worst
            plot_groups = {
                "Random_Samples": np.random.choice(metrics_df.index, n_samples, replace=False),
                "Best_R2_Samples": metrics_df.nlargest(n_samples, 'r2').index.values,
                "Worst_R2_Samples": metrics_df.nsmallest(n_samples, 'r2').index.values,
            }

            # Plot each group
            for name, indices in plot_groups.items():
                # Load fine-grid data from memmap
                v_fine_mm = np.memmap(v_fine_path, dtype=np.float16, mode='r').reshape(-1, n_fine)
                i_fine_mm = np.memmap(i_fine_path, dtype=np.float16, mode='r').reshape(-1, n_fine)

                # For each sample
                for idx in indices:
                    v_slice, i_pred_slice = v_slices[idx], preds[idx]
                    v_fine, i_fine = v_fine_mm[idx], i_fine_mm[idx]

                    # Plot fine-grid ground truth
                    ax.plot(v_fine, i_fine, 'k-', alpha=0.7, lw=2, label='Actual (Fine)')

                    # Reconstruct prediction using PCHIP
                    i_pred_fine = PchipInterpolator(v_slice, i_pred_slice, extrapolate=False)(v_fine)
                    ax.plot(v_fine, i_pred_fine, 'r--', lw=2, label='Predicted (Reconstructed)')

                    # Plot coarse-grid points
                    ax.plot(v_slice, i_true_slice, 'bo', ms=6, label='Actual Points')
                    ax.plot(v_slice, i_pred_slice, 'rx', ms=6, mew=2, label='Predicted Points')

generate_summary_plots(results: list[dict], output_dir: Path):
  Purpose: Compare multiple experiment runs (box plots)
  Metrics: R² median, MAE median, Voc/Isc errors
  Implementation:
    df = pd.DataFrame([r for r in results if 'error' not in r])
    metrics_to_plot = {
        'Median R² Score': 'r2_median',
        'Median Abs. MAE': 'mae_median',
        'Median Voc Error (V)': 'voc_error_abs_median',
        'Median Isc Error (mA/cm²)': 'isc_error_abs_median'
    }
    for key in metrics_to_plot.values():
        sns.boxplot(data=df, x=key, y='run_name', orient='h', palette='viridis')

================================================================================
INFERENCE BENCHMARKING (SPEED & FLOPS)
================================================================================
File: Add to `inference.py` or create `benchmark.py`

Benchmark Function:
  def benchmark_inference(model, device, batch_size=128, n_warmup=20, n_iters=200):
      model.eval()
      param_dim, seq_len = model.hparams.model['param_dim'], model.hparams.dataset['pchip']['seq_len']
      X_dummy = torch.randn(batch_size, param_dim, device=device)
      V_dummy = torch.randn(batch_size, seq_len, device=device)

      # Warmup
      with torch.no_grad():
          for _ in range(n_warmup):
              _ = model(X_dummy, V_dummy)

      # Timing
      if device.type == 'cuda':
          starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
          starter.record()
          for _ in range(n_iters):
              _ = model(X_dummy, V_dummy)
          ender.record()
          torch.cuda.synchronize()
          total_ms = starter.elapsed_time(ender)
      else:
          t0 = time.perf_counter()
          for _ in range(n_iters):
              _ = model(X_dummy, V_dummy)
          total_ms = (time.perf_counter() - t0) * 1000.0

      avg_ms = total_ms / n_iters
      throughput = batch_size / (avg_ms / 1000)

      print(f"\nDevice: {device}  Batch size: {batch_size}")
      print(f"  → Total time for {n_iters} runs: {total_ms:.1f} ms")
      print(f"  → Average latency per forward: {avg_ms:.3f} ms")
      print(f"  → Throughput: {throughput:.0f} samples/s\n")

      return avg_ms, throughput

Usage:
  model = load_trained_model('models/ctrl_point_model.pt')
  benchmark_inference(model, device='cuda', batch_size=128)

FLOP Counting (optional):
  Use pytorch-flops or thop:
    from thop import profile
    flops, params = profile(model, inputs=(X_dummy, V_dummy))
    print(f"FLOPs: {flops/1e9:.2f}G, Params: {params/1e6:.2f}M")

================================================================================
DIRECT CURVE V2: HIGH-ACCURACY ARCHITECTURE (NEW IN v4.0)
================================================================================
File: `models/direct_curve_v2.py`

TARGET: >99% R² accuracy (inspired by Zbinden et al. 2026 R²=0.996 and Toprak 2025 R>0.9996)

MOTIVATION - Why V2?
  The original Split-Spline approach with control points + PCHIP has a fundamental
  information bottleneck: K control points cannot fully represent the 45-point curve.
  This limits accuracy regardless of how well the control points are predicted.

KEY IMPROVEMENTS OVER V1 (CONTROL POINT + PCHIP):

1. DIRECT 45-POINT PREDICTION (no control point bottleneck)
   - Network directly predicts all 45 curve points
   - Removes information loss from PCHIP interpolation
   - Each curve point can be optimized independently

2. PARAMETER RECONSTRUCTION LOSS (Zbinden's "latent loss")
   - Network also reconstructs 6 key physics parameters from learned features
   - Forces encoder to learn physically meaningful representations
   - Parameters: mue_P, muh_P, Gavg, Aug, Brad, SRV_E
   - These are the most important physics drivers of J-V characteristics

3. HARD MONOTONICITY via cumulative decrements
   - Network predicts positive decrements (softplus activation)
   - Curve = Jsc - cumsum(decrements) * Jsc / sum(decrements)
   - Guarantees monotonically decreasing J-V curve by construction
   - Endpoint constraint: curve starts at Jsc, ends near 0 at Voc

4. REGION-WEIGHTED LOSS
   - Higher weight on knee region (~0.75 * Voc) where curve shape changes rapidly
   - Endpoint constraints (Jsc at V=0, ~0 at Voc)
   - Smoothness penalty for non-physical oscillations

ARCHITECTURE:

DirectCurveNetV2 (core model, takes Jsc as input):
  Input: features (102-dim) + Jsc (from pretrained LGBM)
  Encoder: MLP with residual connections
    - Layers: configurable (default 4)
    - Hidden dim: configurable (default 512)
    - Activation: SiLU
    - Normalization: LayerNorm
    - Dropout: 0.1
  Curve Head: Linear → Softplus → 45-point decrements
    - cumsum gives monotonically increasing sequence
    - Normalize by total → fraction in [0, 1]
    - Curve = Jsc * (1 - fraction)
  Parameter Head (optional): Linear → 6 reconstructed parameters
  Output: 45-point curve, optionally Voc + reconstructed parameters

DirectCurveNetV2WithVoc (full model, predicts Voc jointly):
  Input: features (102-dim) + Jsc (from pretrained LGBM)
  Encoder: Same as above
  Voc Head: Linear → Softplus (ensures positive)
  Curve Head: Same as above (uses predicted Voc for knee weighting)
  Parameter Head: Same as above
  Output: 45-point curve + Voc + reconstructed parameters

LOSS FUNCTIONS:

DirectCurveLossV2:
  L_total = L_curve + λ_endpoint * L_endpoint + λ_smooth * L_smooth + λ_params * L_params

  L_curve: Region-weighted MSE on 45 points
    - Knee region (0.6*Voc to 0.9*Voc): 3x weight
    - Elsewhere: 1x weight

  L_endpoint: Hard endpoint constraint
    - |pred[0] - Jsc|² (must start at Jsc)
    - |pred[-1] - 0|² (must end near 0 at Voc)

  L_smooth: Second derivative penalty
    - Σ|pred[i+1] - 2*pred[i] + pred[i-1]|²
    - Prevents non-physical oscillations

  L_params: Parameter reconstruction MSE
    - MSE(reconstructed_params, true_params) on normalized scale
    - Only active if param_weight > 0

DirectCurveLossV2WithVoc (adds Voc loss):
  L_total = L_curve + L_voc + L_endpoint + L_smooth + L_params
  L_voc = MSE(pred_voc, true_voc) * voc_weight

CONFIG CLASSES:

DirectCurveNetV2Config:
  - input_dim: Number of input features (default 102)
  - n_curve_points: Output curve points (default 45)
  - hidden_dim: Encoder hidden dimension (default 512)
  - n_layers: Number of encoder layers (default 4)
  - dropout: Dropout rate (default 0.1)
  - use_residual: Use residual connections (default True)
  - n_params_to_reconstruct: Number of physics params (default 6)

HELPER FUNCTIONS:

get_params_for_reconstruction(X_raw, param_names):
  Extracts the 6 key physics parameters from raw input:
    - mue_P: Electron mobility in perovskite
    - muh_P: Hole mobility in perovskite
    - Gavg: Generation rate
    - Aug: Auger recombination coefficient
    - Brad: Radiative recombination coefficient
    - SRV_E: Surface recombination velocity (ETL side)

normalize_params_for_loss(params, param_names):
  Log-normalizes parameters for balanced loss contribution:
    - log1p transform compresses orders of magnitude
    - Zero-mean, unit-variance scaling

CLI USAGE:

  python train.py \
    --params LHS_parameters_m.txt LHS_parameters_m_300k.txt \
    --iv IV_m.txt IV_m_300k.txt \
    --output outputs_direct_v2/ \
    --device cuda \
    --train-curves \
    --direct-curve-v2 \        # <-- New flag for V2
    --drop-weak-features \
    --drop-multicollinear \
    --hpo-trials-nn 5 \
    --hpo-trials-lgbm 100

SLURM SCRIPT:

File: `slurm_direct_v2.sh`
  - Uses --direct-curve-v2 flag
  - HPO_TRIALS_NN=5 (Voc NN only)
  - HPO_TRIALS_LGBM=100 (for Jsc, Vmpp, Jmpp)
  - HPO_TIMEOUT=7200 (2 hours per model)
  - Time limit: 12 hours

  Output files:
    - $OUT_DIR/hpo_results.json (reusable for future runs)
    - $OUT_DIR/models/direct_curve_v2_model.pt
    - $OUT_DIR/metrics.json
    - $OUT_DIR/training_summary.json

  To skip HPO in future runs:
    sbatch slurm_direct_v2.sh $OUT_DIR/hpo_results.json

TRAINING FLOW (train_direct_curve_model_v2):

1. Extract Jsc values from true curves (first point)
2. Extract 6 physics parameters for reconstruction loss
3. Normalize parameters for balanced loss
4. Initialize DirectCurveNetV2WithVoc with config
5. Training loop:
   a. Forward pass: features + Jsc → curve + Voc + params
   b. Compute region-weighted curve loss
   c. Compute Voc MSE loss
   d. Compute parameter reconstruction loss
   e. Compute smoothness penalty
   f. Backprop combined loss
6. Validation: Track R² on hold-out set
7. Save best model checkpoint

COMPARISON: V1 vs V2

| Aspect              | V1 (Split-Spline)        | V2 (Direct Curve)        |
|---------------------|--------------------------|--------------------------|
| Output              | K control points + PCHIP | 45 points directly       |
| Monotonicity        | PCHIP guarantees         | Cumulative decrements    |
| Information         | Bottleneck at K points   | Full 45-point capacity   |
| Physics encoding    | None                     | Parameter reconstruction |
| Knee handling       | Split at Vmpp            | Region weighting         |
| Typical R²          | ~0.95-0.98               | Target >0.99             |

EXPECTED IMPROVEMENTS:
  - 2-5% higher R² (removes control point bottleneck)
  - Better knee region accuracy (explicit weighting)
  - More physically meaningful features (parameter reconstruction)
  - Comparable inference speed (same encoder, different head)

================================================================================
KEY IMPLEMENTATION DETAILS FOR REPLICATION (UPDATED v4.0)
================================================================================
Voltage grid: `config.py` hardcodes `V_GRID` (45 points)
Control points: Default K=4 per region → 6 knots total per region (V1 only)
Cumulative scaling: build_knots uses cumsum for guaranteed monotonicity
PCHIP slopes: Fritsch-Carlson with eps=1e-12 safe division
Continuity: Finite differences with local grid spacing
Kendall init: log_sigma = torch.zeros(1) → σ=1.0 initially
Feature mask: Boolean array in checkpoint, applied before physics computation
Hard clamp training: Default True, fixes train-test distribution shift
Logging: TrainingLogger accumulates logs, saves all on completion

**NEW in v3.0:**
Curve normalization: 2.0 * (curve / isc) - 1.0 → [-1, 1] (decouples magnitude/shape)
Denormalization: (normalized + 1.0) / 2.0 * isc → [0, isc] (for evaluation)
Parameter transform: log1p → RobustScaler → MinMaxScaler → [-1, 1] (handles outliers)
Preprocessing class: PVDataPreprocessor (fit on train, transform val/test)
CRITICAL: Curve reconstruction uses PREDICTED anchors (not raw inputs) ✓ Correct
Training space: Normalized [-1, 1] for better gradients
Evaluation space: Absolute [0, isc] for interpretability
Validation: Round-trip error < 1e-4, normalized range [-1.1, 1.1]

**NEW in v4.0:**
DirectCurveNetV2: Direct 45-point prediction (no control point bottleneck)
Parameter reconstruction: 6 key physics parameters (latent loss)
Cumulative decrements: Guaranteed monotonicity by construction
Region weighting: 3x weight on knee region (0.6-0.9 * Voc)
CLI flag: --direct-curve-v2 for new architecture
SLURM script: slurm_direct_v2.sh for cluster runs

================================================================================
END OF DOCUMENTATION (v4.0 - WITH DIRECT CURVE V2 HIGH-ACCURACY)
================================================================================

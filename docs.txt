PHYSICS-CONSTRAINED SPLIT-SPLINE J-V CURVE RECONSTRUCTION FOR PEROVSKITE SOLAR CELLS
CONDENSED IMPLEMENTATION PLAN (AS-BUILT PIPELINE v2.0 - ROBUST LOGGING EDITION)

================================================================================
PROBLEM STATEMENT
================================================================================
Objective: Predict full J-V curves (45 voltage-current pairs) for perovskite solar cells from 31 material parameters with >99.9% accuracy. Challenge: Direct MLP prediction ignores physical boundaries (J cannot exceed Jsc, V cannot exceed Voc) and produces non-monotonic artifacts at the maximum power point (MPP) "knee". Solution: Multi-head neural network that explicitly predicts physical anchors (Jsc, Voc, Vmpp, Jmpp) with hard constraint projection, then reconstructs curve shape using split PCHIP interpolation at MPP to decouple transport-limited (Region 1: 0→Vmpp) from recombination-limited physics (Region 2: Vmpp→Voc).

================================================================================
DATA & PHYSICS CONTEXT
================================================================================
Files: `data.py` handles loading, `config.py` defines grid/constants.
Input: 31 COMSOL-simulated material parameters (layer thicknesses, mobilities, carrier lifetimes, doping, work functions, recombination coefficients).
Curves: 45-point J-V characteristics on non-uniform voltage grid [0, 0.1, 0.2, 0.3, 0.4, 0.425, 0.45, ..., 1.4V] totaling 45 evaluation points (`V_GRID` in `config.py`).
Physics features: 71 engineered features computed in `features.py:compute_all_physics_features` from raw params. Includes debye_length, diffusion_length, mobility_ratios, dos_ratios, energy_differences, generation_rate, recombination_timescales, field_strengths, einstein_relations, ideality_factor_proxies, plus thermodynamic ceilings (jsc_ceiling from q×G×L, voc_ceiling from Shockley-Queisser).

Feature validation: Two-stage filtering implemented:
  1. Pearson correlation analysis (`features.py:validate_physics_features`) drops weak features (|r|<0.3 vs targets)
  2. Multicollinearity check (`logging_utils.py:compute_multicollinearity`) identifies highly correlated feature pairs (|r|>0.85)

Masking logic: `train.py:ScalarPredictorPipeline._apply_feature_mask` applies both filters.

================================================================================
IMPLEMENTED ARCHITECTURE (AS-BUILT)
================================================================================
File: `models/voc_nn.py`

Primary curve model (current): UnifiedSplitSplineNet (split‑spline, multi‑head).
  Inputs:
    - Features: 31 raw params + 71 physics features (derived only from the 31 params)
  Outputs:
    - Anchors: [Jsc, Voc, Vmpp, Jmpp] with hard projection
    - Region 1 control points (0→Vmpp)
    - Region 2 control points (Vmpp→Voc)
  Reconstruction:
    - Jsc‑normalized split PCHIP with knots clustered around Vmpp
    - Endpoint enforcement at V=0, V=Vmpp, V=Voc
    - ~12 knots total (K≈5 per region + endpoints)

ControlPointNet (control‑point‑only) remains available but is not the default.

CONFIGURABLE CONTROL POINTS (v2.0):
  Default K=4 (reduced from 6 for simpler models)
  Set via --ctrl-points CLI argument
  Each region has K+2 knots total (2 anchors + K interior points)

================================================================================
HARD CONSTRAINT PROJECTION (WITH VIOLATION LOGGING)
================================================================================
File: `models/voc_nn.py` function `physics_projection`

Implemented: Differentiable clamping via torch.clamp ensuring:
  Jsc > 1e-6 (strict positivity)
  Voc > 1e-6 (strict positivity)
  1e-6 < Vmpp < Voc - 1e-6 (strict ordering)
  1e-6 < Jmpp < Jsc - 1e-6 (strict ordering)

NEW IN v2.0 - Violation Logging:
  physics_projection(..., return_violations=True) returns violation counts BEFORE clamping:
    - jsc_negative: count of Jsc < 1e-6
    - voc_negative: count of Voc < 1e-6
    - vmpp_exceeds_voc: count of Vmpp >= Voc
    - jmpp_exceeds_jsc: count of Jmpp >= Jsc

  This diagnoses how often the network "wants" to violate constraints during training.
  Logged per-epoch to constraint_violations.csv via TrainingLogger.

================================================================================
SPLIT-SPLINE RECONSTRUCTION LAYER (DIFFERENTIABLE PCHIP)
================================================================================
File: `models/reconstruction.py`

build_knots() constructs monotonic voltage-current knot sequences:
  CUMULATIVE SCALING (guarantees monotonicity):
    - ctrl values are sigmoid outputs in [0,1]
    - cumsum gives strictly increasing sequence
    - Normalizing by final cumsum ensures exact endpoint matching
  - j_interior interpolates between Jsc→Jmpp (Region 1) or Jmpp→J_end (Region 2)

  Region 1: K+2 knots from (V=0, J=Jsc) through K interior points to (V=Vmpp, J=Jmpp)
  Region 2: K+2 knots from (V=Vmpp, J=Jmpp) through K interior points to (V=Voc, J=J_end)

NEW IN v2.0 - Monotonicity Violation Tracking:
  build_knots(..., return_violation_counts=True) returns:
    - region1_violations: samples with non-monotonic J in Region 1
    - region2_violations: samples with non-monotonic J in Region 2
    - total_samples: batch size

  Logged to monotonicity.csv for debugging spline construction issues.

Interpolation: `pchip_interpolate_batch` implements Fritsch-Carlson PCHIP in PyTorch.
Knot placement: Vmpp‑clustered (denser near the knee in both regions).
Stitching: Curves split at Vmpp, concatenated with mask-based selection.
Endpoint enforcement (new):
  - After PCHIP, set J(V=0)=Jsc, set J(Vmpp)=Jmpp, and clamp J to J_end at V>=Voc
  - Normalized curves use J_end=-1 (Isc-normalized space)
  - Absolute curves use J_end=0
Tail handling: L_tail = relu(J[v>Voc]).mean() + hard clamp at inference.

================================================================================
LOSS FUNCTION (KENDALL MULTI-TASK + CONTINUITY)
================================================================================
File: `train.py` class `MultiTaskLoss`

Kendall multi-task learning with learnable log-variances:
  Parameters: log_sigma_anchor, log_sigma_curve (trainable)
  Formula: L = L_anchor/(2σ_a²) + log(σ_a) + L_curve/(2σ_c²) + log(σ_c)

  L_anchor = MSE(pred_anchors, true_anchors) on 4 scalars
  L_curve = MSE(reconstructed_curve, true_curve) on 45 points (Jsc‑normalized)

NEW IN v2.0 - Sigma Logging:
  MultiTaskLoss tracks:
    - sigma_anchor, sigma_curve: actual σ values (exp of log_sigma)
    - sigma_ratio: σ_anchor / σ_curve (detect task imbalance)
    - task_imbalance: True if ratio > 10 or < 0.1

  Logged per-epoch to multitask_losses.csv.
  Console warnings when task imbalance detected.

Continuity regularization: `models/reconstruction.py:continuity_loss`
  L_cont = (J1(Vmpp) - J2(Vmpp))² + (dJ1/dV - dJ2/dV)²
  Weight configurable via --continuity-weight (default 0.5, try 0.1-1.0)

Total loss: L_total = L_weighted + λ_cont × L_cont

================================================================================
SCALAR INPUTS (VOC/VMPP) AS EXTERNAL TXT FILES
================================================================================
Scalar inputs (Voc, Vmpp) are consumed from external TXT files so they can be
swapped with outputs from separate NN/LGBM predictors. The curve pipeline only
reads these TXT inputs; generation is intentionally isolated.

================================================================================
DATA FILTERING (UNPHYSICAL CURVE REMOVAL)
================================================================================
Only very unphysical curves are removed as part of data generation quality control
(Latin Hypercube Sampling over a Bayesian distribution of COMSOL sweeps across 31
coupled PDE parameters). Filtering is limited to extreme FF/Vmpp cases that violate
basic device physics.

================================================================================
TRAIN-TEST MISMATCH FIX (CRITICAL v2.0 CHANGE)
================================================================================
PROBLEM IDENTIFIED:
  Original: Soft penalty during training, hard clamp only at inference
  Result: Network never learns boundary behavior, performance drops at test time

SOLUTION IMPLEMENTED:
  Hard clamping (torch.clamp) now applied DURING TRAINING by default
  Network learns correct behavior at constraint boundaries
  No train-test distribution shift

CLI Control:
  Default: Hard clamp during training (use_hard_clamp_training=True)
  Override: --soft-clamp-training to use soft penalties (not recommended)

================================================================================
MULTICOLLINEARITY DETECTION (NEW IN v2.0)
================================================================================
File: `logging_utils.py`

Functions:
  compute_multicollinearity(features, threshold=0.85):
    Returns correlation matrix and list of highly correlated pairs

  suggest_features_to_drop(corr_matrix, feature_names, target_correlations, threshold):
    When two features are highly correlated, keeps the one with higher target correlation

Integration in train.py:
  ScalarPredictorPipeline.run_feature_validation() computes multicollinearity
  If --drop-multicollinear flag set, removes redundant features
  Report saved to multicollinearity.json

Threshold configurable via --multicollinearity-threshold (default 0.85)

================================================================================
LOGGING INFRASTRUCTURE (NEW IN v2.0)
================================================================================
File: `logging_utils.py`

TrainingLogger class provides centralized logging:

Dataclasses for structured logging:
  ConstraintViolationLog: epoch, batch, jsc_negative, voc_negative, vmpp_exceeds_voc, jmpp_exceeds_jsc, total_samples
  MultiTaskLossLog: epoch, loss_anchor, loss_curve, sigma_anchor, sigma_curve, loss_continuity, loss_tail, loss_total
  MonotonicityLog: epoch, batch, region1_violations, region2_violations, total_samples
  ModelComparisonMetrics: model_name, mse_full_curve, mse_region1/2, mae_jsc/voc/vmpp/jmpp, mape_ff, violations_*, inference_time_ms

Output files generated:
  constraint_violations.csv   - Per-epoch constraint violation attempts
  multitask_losses.csv        - Per-epoch sigma values and loss components
  monotonicity.csv            - Spline knot monotonicity violations
  multicollinearity.json      - Feature correlation analysis
  model_comparison.json       - Split-Spline vs CVAE metrics
  model_comparison.md         - Markdown comparison table
  training_summary.json       - Overall training summary with duration, final sigmas, violation rates

TrainingLogger.generate_comparison_table() produces formatted markdown:
  | Model | MSE Full | MSE R1 | MSE R2 | FF MAPE (%) | Violations/1000 | Time (ms) |

================================================================================
HYPERPARAMETER OPTIMIZATION (SIMPLIFIED IN v2.0)
================================================================================
File: `hpo.py`

SIMPLIFIED SEARCH SPACES (narrower ranges for faster convergence):

VOC NN (sample_voc_nn_config):
  - Layers: 2-3 (was 2-4)
  - Hidden dim: 256-768 (unchanged)
  - Learning rate: 5e-5 to 1e-3 (narrower, was 1e-5 to 1e-2)
  - Normalization: LayerNorm always (removed BatchNorm option)
  - Activation: gelu or silu only (removed relu, swish, mish)
  - Dropout: 0.1-0.3 (unchanged)
  - Weight decay: 1e-5 to 1e-2 (unchanged)

LightGBM (sample_lgbm_config):
  - Boosting: GBDT only (removed DART - too slow)
  - Learning rate: 0.01-0.1 (narrower, was 0.005-0.2)
  - Num leaves: 31-255 (unchanged)
  - Max depth: 6-12 (unchanged)
  - Feature/bagging fraction: 0.7-1.0 (unchanged)
  - Min child samples: 5-50 (unchanged)

HPO Trials (configurable):
  --hpo-trials-nn: Default 50 (was 100)
  --hpo-trials-lgbm: Default 100 (was 200)
  --hpo-timeout: Default 7200 seconds (2 hours per model)

FIX: Added Jmpp LGBM to run_full_hpo() (was missing)
NEW: Added Voc LGBM HPO with ceiling ratio target

================================================================================
CVAE BASELINE (WITH COMPARISON LOGGING)
================================================================================
File: `models/cvae.py`

Conditional Variational Autoencoder for comparison only:
  Encoder: curve + params → latent_dim=16 via mu/logvar
  Decoder: latent + params → reconstructed curve
  Loss: cvae_loss = MSE_reconstruction + β×KL_divergence (β=0.001)

NEW IN v2.0 - Comparison Logging:
  evaluate_cvae() now creates ModelComparisonMetrics:
    - mse_full_curve, mse_region1, mse_region2
    - mae_jsc, mae_voc, mae_vmpp, mae_jmpp
    - mape_ff
    - violations_jsc_negative, violations_voc_negative, etc.
    - violations_j_exceeds_jsc (J > Jsc anywhere on curve)
    - inference_time_ms

  Logged alongside Split-Spline metrics for side-by-side comparison.
  The production curve model is ControlPointNet, not CVAE.

================================================================================
TRAINING PIPELINE
================================================================================
File: `train.py` class `ScalarPredictorPipeline`

Constructor options (updated for v2.0):
  - output_dir: Where to save models and logs
  - device: 'cuda' or 'cpu'
  - drop_weak_features: Remove features with |r| < 0.3
  - multicollinearity_threshold: Default 0.85
  - drop_multicollinear: Remove redundant features
  - continuity_weight: Default 0.5 (try 0.1-1.0)
  - ctrl_points: Default 4 (try 4-6)
  - use_hard_clamp_training: Default True (fixes train-test mismatch)
  - log_constraint_violations: Default True
  - verbose_logging: Default True

Training flow:
  1. Load data, compute physics features
  2. Run feature validation (correlation + multicollinearity)
  3. Train Voc NN + Voc LGBM, select best on val
  4. Train Jsc LGBM
  5. Train Vmpp/Jmpp/FF LGBMs using predicted intermediates (no oracle inputs)
  6. Train ControlPointNet for curve reconstruction using predicted anchors
  7. Optionally train CVAE baseline (comparison only)
  8. Evaluate all models using predicted-chain inputs
  9. Save all logs and checkpoints

train_curve_model() loop:
  - Hard clamping during training (if use_hard_clamp_training=True)
  - Log sigma values and constraint violations per epoch
  - Track monotonicity violations from build_knots
  - Configurable continuity weight

================================================================================
CLI ARGUMENTS (UPDATED FOR v2.0)
================================================================================
python train.py [options]

Data options:
  --params FILE          LHS parameters file
  --iv FILE              IV curves file
  --output DIR           Output directory

Feature options:
  --drop-weak-features          Drop features with weak target correlation
  --drop-multicollinear         Drop multicollinear features
  --multicollinearity-threshold FLOAT  Threshold for multicollinearity (default 0.85)

Training options:
  --train-curves         Train split-spline curve model
  --train-cvae           Train CVAE baseline for comparison
  --device DEVICE        cuda or cpu (default: cuda)

Model options:
  --ctrl-points INT      Number of control points per region (default 4)
  --continuity-weight FLOAT  Weight for continuity loss (default 0.5)
  --soft-clamp-training  Use soft penalties instead of hard clamp (not recommended)

HPO options:
  --hpo-trials-nn INT    HPO trials for neural networks (default 50)
  --hpo-trials-lgbm INT  HPO trials for LightGBM (default 100)
  --hpo-timeout INT      HPO timeout in seconds (default 7200)

Logging options:
  --no-constraint-logging  Disable constraint violation logging
  --quiet                  Reduce console output

================================================================================
OUTPUT FILES GENERATED
================================================================================
After training with --train-curves --train-cvae:

Models:
  models/ctrl_point_model.pt  - ControlPointNet checkpoint (primary curve model)
  models/curve_model.pt       - Legacy UnifiedSplitSplineNet checkpoint (if trained)
  models/cvae_model.pt        - CVAE checkpoint (comparison only)
  models/voc_nn.pt            - Voc neural network
  models/voc_lgbm.txt         - Voc LightGBM
  models/jsc_lgbm.txt         - Jsc LightGBM
  models/vmpp_lgbm.txt        - Vmpp LightGBM
  models/jmpp_lgbm.txt        - Jmpp LightGBM
  models/ff_lgbm.txt          - FF LightGBM
  models/preprocessor.joblib  - Parameter/curve preprocessing state

Logs (new in v2.0):
  constraint_violations.csv   - How often network tried to violate physics
  multitask_losses.csv        - Sigma values per epoch (detect task imbalance)
  monotonicity.csv            - Spline knot monotonicity violations
  multicollinearity.json      - Feature correlation analysis
  model_comparison.json       - Split-Spline vs CVAE raw metrics
  model_comparison.md         - Formatted comparison table
  training_summary.json       - Duration, final sigmas, overall violation rate

Metrics:
  metrics.json                - Final test set metrics

================================================================================
SLURM SCRIPT (UPDATED FOR v2.0)
================================================================================
File: `slurm_curve_pipeline.sh`

Configuration variables:
  HPO_TRIALS_NN=50            # Reduced due to simplified search space
  HPO_TRIALS_LGBM=100         # Reduced due to simplified search space
  HPO_TIMEOUT=7200            # 2 hours per model
  CONTINUITY_WEIGHT=0.5       # Try 0.1, 0.5, or 1.0
  CTRL_POINTS=4               # Simplified from 6

Time limit: 30 hours (for unattended overnight runs)

Full command:
  python train.py \
    --params "$WORK_DIR/LHS_parameters_m.txt" \
    --iv "$WORK_DIR/IV_m.txt" \
    --output "$OUT_DIR" \
    --device cuda \
    --train-curves \
    --train-cvae \
    --drop-weak-features \
    --drop-multicollinear \
    --hpo-trials-nn $HPO_TRIALS_NN \
    --hpo-trials-lgbm $HPO_TRIALS_LGBM \
    --hpo-timeout $HPO_TIMEOUT \
    --continuity-weight $CONTINUITY_WEIGHT \
    --ctrl-points $CTRL_POINTS

================================================================================
INFERENCE PIPELINE
================================================================================
File: `inference.py`

Command: python inference.py --predict-curve --input params.csv --output curve_predictions.csv

Workflow:
  1. Load checkpoint (model weights, v_grid, feature_mask, normalization stats)
  2. Load preprocessor (param scaling + curve normalization)
  3. Compute 71 physics features from 31 input params
  4. Apply feature mask (weak features + multicollinear features dropped)
  5. Predict anchors using NN/LGBMs (Voc → Jsc → Vmpp → Jmpp)
  6. ControlPointNet predicts control points from (features + predicted anchors)
  7. Reconstruct curve with predicted anchors + control points (normalized PCHIP)
  8. Denormalize by predicted Jsc to absolute space

Uncertainty (--uncertainty flag): MC-Dropout with n_samples=100

================================================================================
DEBUGGING GUIDE (NEW IN v2.0)
================================================================================

If training stalls or performs poorly, check these logs:

1. SIGMA IMBALANCE (multitask_losses.csv):
   - If sigma_anchor << sigma_curve: Anchor task too easy, network ignoring curve
   - If sigma_curve << sigma_anchor: Curve task too easy, anchors dominating
   - Healthy: Both sigmas in similar range (0.3-1.0)

2. CONSTRAINT VIOLATIONS (constraint_violations.csv):
   - High jsc_negative or voc_negative: Network predicting negative values
   - High vmpp_exceeds_voc: Ordering constraint not learned
   - Should decrease over epochs as network learns

3. MONOTONICITY VIOLATIONS (monotonicity.csv):
   - Non-zero region1_violations or region2_violations: Spline construction issues
   - Should be zero with cumulative scaling

4. MULTICOLLINEARITY (multicollinearity.json):
   - Many high-correlation pairs: Redundant features causing instability
   - Use --drop-multicollinear to address

5. TASK IMBALANCE (console warnings):
   - "[!] Anchor task too easy" or "[!] Curve task too easy"
   - May need to adjust continuity weight or check data

================================================================================
VALIDATION CHECKLIST
================================================================================
After training, verify:
  [ ] constraint_violations.csv shows decreasing violation rate over epochs
  [ ] multitask_losses.csv shows balanced sigma values (ratio between 0.1-10)
  [ ] monotonicity.csv shows zero violations
  [ ] model_comparison.md shows Split-Spline beats CVAE on violations
  [ ] training_summary.json shows reasonable duration and final metrics
  [ ] metrics.json shows MSE < 0.001, FF MAPE < 5%

================================================================================
CONV-BASED I-V RECONSTRUCTION (v5.0 — PRIMARY MODEL)
================================================================================
File: `train_attention_tcn.py`, Pipeline: `slurm_tcn_master_pipeline.sh`

KEY FINDING: Dilated 1D convolution (no attention) is the best current
candidate. TCN-style causality is not required for this task and is now treated
as a secondary baseline.

Current evidence snapshot from `atcnresults.csv` (limited seeds):
  - Conv-NoAttn (n=2):            MAE=5.543, R2=0.98299
  - Conv-Dilated-NoAttn (n=1):    MAE=5.657, R2=0.98121
  - TCN-DilatedConv-NoAttn (n=3): MAE=6.006, R2=0.98242
Note: Conv-dilated remains the paper flagship based on architecture rationale
and early results; finalize with balanced 3-seed reruns per config.

Architecture overview (PhysicsIVSystem, PyTorch Lightning):
  1. param_mlp: 31 raw params + 2 scalars (Voc, Vmpp from TXT) → dense [256→128→128]
  2. Gaussian RBF positional encoding of 8-point PCHIP voltage positions
  3. Conditioning: param embedding broadcast across sequence, concatenated with RBF
  4. Dilated Conv blocks (2 layers):
     - Non-causal 1D convolution (kernel=5, dilation=[1,2], filters=[128,64])
     - BatchNorm1d + GELU + Dropout(0.036)
  5. Output head: Linear(64→1) per-position current prediction

Why this is physically plausible for this dataset:
  - The 8-point PCHIP slice is local around MPP; bidirectional local context helps.
  - Dilation captures shoulder/knee scale interactions without recurrent overhead.
  - No hysteresis modeling requirement here; strict temporal causality is less critical.

Attention (brief negative control):
  - Tested only as sanity/ablation.
  - Consistently worsened MAE in matched setups; keep mention minimal in paper.

Physics loss components:
  L_total = MSE + λ_mono × L_monotonicity + λ_conv × L_convexity + λ_curv × L_curvature
  - MSE: standard reconstruction loss on 8-point I-V predictions
  - Monotonicity: penalizes ΔI > 0 (current should decrease with voltage)
  - Convexity: penalizes second derivative sign violations near the knee
  - Excess curvature: penalizes unrealistic oscillations

Data pipeline (must use BOTH datasets):
  - 100k source set: 66k usable samples after COMSOL-stage culling
  - 300k source set: 176k usable samples after COMSOL-stage culling
  - Training uses concatenated 100k+300k cleaned sets by default
  - Scalars (Voc, Vmpp) loaded from external TXT files (swappable with predicted)
  - PCHIP-based 8-point voltage slices around MPP
  - Fine-grid (2000-point) ground truth stored as float16 memmaps for plotting
  - Preprocessing: Log1p → RobustScaler → MinMaxScaler

Training config (current):
  - Optimizer: AdamW, cosine schedule, max epochs 100
  - Batch size: 128 default (plus dedicated batch-size sweep)
  - Early stopping: patience 20 on val_loss
  - bfloat16 mixed precision

Tier 0 experiments (10 configs × 3 seeds = 30 runs):
  T0-1:  Conv + dilation, no attention     — MAIN MODEL
  T0-2:  Conv no dilation, no attention    — isolates dilation contribution
  T0-3:  Dilated TCN, no attention         — temporal/causal baseline
  T0-4:  Pointwise (1×1), no attention     — position-independent baseline
  T0-5:  Conv + dilation + attention       — negative control
  T0-6:  TCN + dilation + attention        — negative control
  T0-7:  No scalars (params only)          — scalar conditioning value
  T0-8:  100k only                         — data scaling effect
  T0-9:  200 epochs                        — convergence check
  T0-10: Batch size 512                    — throughput/generalization tradeoff

Pipeline files:
  slurm_tcn_master_pipeline.sh  — Master SLURM script (experiments + figures)
  train_attention_tcn.py        — Training script (all architecture variants)
  tcn_collect_results.py        — Aggregate results across experiments
  tcn_generate_figures.py       — Generate all paper figures and tables
  tcn_analysis.py               — Physics/Jacobian/sensitivity analysis

================================================================================
WHAT REMAINS (NEXT STEPS FOR PUBLICATION)
================================================================================
Ablation studies: Conv/TCN Tier 0 experiments (10 configs × 3 seeds via slurm_tcn_master_pipeline.sh).
OOD evaluation: Bandgap-based extrapolation, leave-one-material-out.
Uncertainty calibration: MC-Dropout intervals, calibration plots.
Experimental validation: NREL database curves, published J-V data.
Sensitivity analysis: Jacobians via autograd, physics sign validation (tcn_analysis.py).

================================================================================
FIGURE ROADMAP (v2 — PAPER ASSETS)
================================================================================
Main figures (high-impact):
  - FIG 1: End-to-end pipeline (COMSOL params + scalar TXT -> preprocessing -> Dilated Conv -> curve)
  - FIG 2: Why dilated convolution works (receptive field + short-sequence inductive bias + metric panel)
  - FIG 3: Side-by-side interpolation fidelity pair using:
    `jv_curve_8_point_reconstruction_example.png` (left) and
    `reconstruction_number_vs_number_of_points.png` (right)
  - FIG 4: Voltage-grid + weighted-error rationale (why knee/MPP region is emphasized)
  - FIG 5: Ablation + attention negative control (paired no-attn->attn degradation)
  - FIG 6: Scalar-to-curve error propagation robustness (Voc/Vmpp noise -> curve MAE/R2)

Figure 3 caption/discussion (ready to use):
  - Caption: "Left: Example J-V curve reconstruction using an 8-point PCHIP
    representation against a 1000-point reference, showing close overlap
    (R^2=0.9939). Right: Reconstruction performance as a function of
    interpolation-point count (4 significant digits). Accuracy saturates rapidly:
    around 10 points reaches approximately 0.999, and 12 points reaches about
    0.9997. This margin is over an order of magnitude beyond the smallest error
    scale and supports robust generalization."
  - Discussion cue: point-budget choice is justified by saturation behavior, not
    by maximal interpolation complexity.

Appendix figures:
  - Layer-level architecture detail
  - Preprocessing decision flow
  - Feature streamlining (71 -> m)
  - Jacobian sensitivity heatmap
  - Parameter perturbation tornado
  - Runtime/throughput breakdown (including optimization effects)
  - Data scaling (100k vs 100k+300k)
  - Best/median/worst reconstruction gallery
  - OOD/ID + uncertainty
  - COMSOL coupled PDE context placeholder (Drift-Diffusion-Poisson; header/shell to fill)
  - Curve representation fidelity (good bowed J-V vs bad concave-up curve; interpolation comparison)

Auto-generated (not bespoke design work):
  - Training/learning curves generated directly from run logs by existing code.

Legacy visuals to de-emphasize/remove:
  - Feasible-set projection geometry tied to split-spline projection
  - Split-spline multi-head equation flow figure
  - Control-point and continuity sweeps as core figures

================================================================================
KEY IMPLEMENTATION DETAILS FOR REPLICATION
================================================================================
Voltage grid: `config.py` hardcodes `V_GRID` (45 points)
Control points: Default K=4 per region → 6 knots total per region
Cumulative scaling: build_knots uses cumsum for guaranteed monotonicity
PCHIP slopes: Fritsch-Carlson with eps=1e-12 safe division
Continuity: Finite differences with local grid spacing
Kendall init: log_sigma = torch.zeros(1) → σ=1.0 initially
Feature mask: Boolean array in checkpoint, applied before physics computation
Hard clamp training: Default True, fixes train-test distribution shift
Logging: TrainingLogger accumulates logs, saves all on completion

================================================================================
DATA PREPROCESSING PIPELINE (v3.0 - KNOWN WORKING NORMALIZATION)
================================================================================
File: `preprocessing.py`

CRITICAL: train.py now uses Isc normalization via PVDataPreprocessor.

Curve Normalization by Isc:
  Formula: normalized = 2.0 * (curve / isc) - 1.0
  - Maps [0, isc] → [-1, 1] range
  - Decouples magnitude (Isc) from shape (easier learning)
  - First point: 2*(isc/isc) - 1 = 1.0
  - Last point (Voc): 2*(0/isc) - 1 = -1.0

  Implementation:
    def normalize_curves_by_isc(curves: np.ndarray) -> tuple:
        isc_values = curves[:, 0].copy()
        isc_safe = np.where(np.abs(isc_values) < 1e-9, 1.0, isc_values)
        normalized = 2.0 * (curves / isc_safe[:, np.newaxis]) - 1.0
        return isc_values, normalized

Denormalization (inverse):
  Formula: curve = (normalized + 1.0) / 2.0 * isc
  - Maps [-1, 1] → [0, isc]
  - MUST be applied before evaluation in absolute space
  - Round-trip: raw → normalize → denormalize → raw (error < 1e-6)

  Implementation:
    def denormalize_curves_by_isc(normalized, isc):
        is_tensor = isinstance(normalized, torch.Tensor)
        if is_tensor:
            isc = isc.unsqueeze(1) if isc.dim() == 1 else isc
        else:
            isc = isc[:, np.newaxis] if isc.ndim == 1 else isc
        return (normalized + 1.0) / 2.0 * isc

Parameter Transformations (RobustScaler + MinMaxScaler + log1p):
  WHY: Material properties span orders of magnitude (10^-5 to 10^27)
  - Simple Z-score fails due to outliers and heavy tails
  - RobustScaler: Uses median + IQR (robust to outliers)
  - MinMaxScaler: Maps to [-1, 1] (symmetric, matches NN activations)
  - log1p for material_properties: Compresses orders of magnitude

  Groups:
    layer_thickness: ['lH', 'lP', 'lE'] → RobustScaler → MinMaxScaler
    material_properties: [mobilities, DOS, energies, eps] → log1p → RobustScaler → MinMaxScaler
    contacts: ['Wlm', 'Whm'] → RobustScaler → MinMaxScaler
    recombination_gen: ['Gavg', 'Aug', 'Brad', lifetimes, velocities] → RobustScaler → MinMaxScaler

  Implementation:
    from sklearn.preprocessing import RobustScaler, MinMaxScaler, FunctionTransformer
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline

    def get_param_transformer(colnames):
        transformers = []
        for group, cols in param_defs.items():
            steps = [('robust', RobustScaler()), ('minmax', MinMaxScaler(feature_range=(-1, 1)))]
            if group == 'material_properties':
                steps.insert(0, ('log1p', FunctionTransformer(func=np.log1p)))
            transformers.append((group, Pipeline(steps), cols))
        return ColumnTransformer(transformers, remainder='passthrough')

PVDataPreprocessor class:
  Combines all preprocessing steps:
  - fit_transform_params(X_train) → fits transformers on training data
  - transform_params(X_val/test) → applies fitted transformers
  - fit_transform_curves(curves_train) → returns (isc, normalized_curves)
  - denormalize_curves(pred_norm, isc) → returns absolute predictions
  - save(filepath) / load(filepath) → for inference

Training Pipeline Changes (train.py):
  1. Fit preprocessor on training set:
       preprocessor = PVDataPreprocessor()
       X_train_transformed = preprocessor.fit_transform_params(X_train_full)
       isc_train, curves_train_norm = preprocessor.fit_transform_curves(curves_train)

  2. In training loop (normalized space):
       pred_curve_norm = reconstruct_curve_normalized(anchors_raw, ctrl1, ctrl2, v_grid)
       loss = mse_loss(pred_curve_norm, target_curves_norm)

  3. In evaluation (denormalize to absolute space):
       pred_curve_norm = reconstruct_curve_normalized(anchors, ctrl1, ctrl2, v_grid)
       pred_curve_abs = denormalize_curves_by_isc(pred_curve_norm, anchors[:, 0])
       mse = ((pred_curve_abs - true_curves_abs) ** 2).mean()

CRITICAL NOTES:
  - Curve reconstruction MUST use PREDICTED anchors (not raw inputs) ✓ Already correct
  - Loss can be computed in normalized space (better gradients)
  - Evaluation MUST be in absolute space for interpretability
  - Round-trip validation: normalize → denormalize should recover original

Validation Function:
  validate_curve_normalization(curves_raw, curves_norm, isc, n_samples=5)
  Checks:
    - Normalized range is ~[-1, 1]
    - Round-trip error < 1e-4
    - First point normalizes to 1.0
    - Last point normalizes to -1.0

Integration Status:
  ✅ preprocessing.py: Complete implementation
  ✅ train.py: Uses PVDataPreprocessor + Isc-normalized curves
  ✅ inference.py: Loads preprocessor + denormalizes curves

Expected Improvements:
  - 10-30% lower MSE (better-scaled gradients)
  - 30-50% faster convergence (normalized space easier to optimize)
  - Better generalization (RobustScaler handles outliers)
  - More stable training (no magnitude/shape coupling)

================================================================================
PLOTTING AND VISUALIZATION (PCHIP RECONSTRUCTION)
================================================================================
File: `train_updated.py` or add to `train.py`

ExamplePlotsCallback (for PyTorch Lightning):
  Purpose: Generate plots of reconstructed curves using PCHIP interpolation
  Features:
    - Plots fine-grid ground truth vs reconstructed predictions
    - Shows actual points (coarse grid) vs interpolated (fine grid)
    - Generates best/worst/random samples based on R² score
    - Logs to TensorBoard

  Implementation:
    class ExamplePlotsCallback(pl.Callback):
        def on_test_end(self, trainer, pl_module):
            # Get predictions and true curves
            preds, trues = pl_module.all_test_preds_np, pl_module.all_test_trues_np

            # Compute R² for ranking
            valid_mask = [i for i in range(len(trues)) if np.var(trues[i]) > 1e-6]
            metrics_df = pd.DataFrame({'r2': [r2_score(trues[i], preds[i]) for i in valid_mask]})

            # Select samples: random, best, worst
            plot_groups = {
                "Random_Samples": np.random.choice(metrics_df.index, n_samples, replace=False),
                "Best_R2_Samples": metrics_df.nlargest(n_samples, 'r2').index.values,
                "Worst_R2_Samples": metrics_df.nsmallest(n_samples, 'r2').index.values,
            }

            # Plot each group
            for name, indices in plot_groups.items():
                # Load fine-grid data from memmap
                v_fine_mm = np.memmap(v_fine_path, dtype=np.float16, mode='r').reshape(-1, n_fine)
                i_fine_mm = np.memmap(i_fine_path, dtype=np.float16, mode='r').reshape(-1, n_fine)

                # For each sample
                for idx in indices:
                    v_slice, i_pred_slice = v_slices[idx], preds[idx]
                    v_fine, i_fine = v_fine_mm[idx], i_fine_mm[idx]

                    # Plot fine-grid ground truth
                    ax.plot(v_fine, i_fine, 'k-', alpha=0.7, lw=2, label='Actual (Fine)')

                    # Reconstruct prediction using PCHIP
                    i_pred_fine = PchipInterpolator(v_slice, i_pred_slice, extrapolate=False)(v_fine)
                    ax.plot(v_fine, i_pred_fine, 'r--', lw=2, label='Predicted (Reconstructed)')

                    # Plot coarse-grid points
                    ax.plot(v_slice, i_true_slice, 'bo', ms=6, label='Actual Points')
                    ax.plot(v_slice, i_pred_slice, 'rx', ms=6, mew=2, label='Predicted Points')

generate_summary_plots(results: list[dict], output_dir: Path):
  Purpose: Compare multiple experiment runs (box plots)
  Metrics: R² median, MAE median, Voc/Isc errors
  Implementation:
    df = pd.DataFrame([r for r in results if 'error' not in r])
    metrics_to_plot = {
        'Median R² Score': 'r2_median',
        'Median Abs. MAE': 'mae_median',
        'Median Voc Error (V)': 'voc_error_abs_median',
        'Median Isc Error (mA/cm²)': 'isc_error_abs_median'
    }
    for key in metrics_to_plot.values():
        sns.boxplot(data=df, x=key, y='run_name', orient='h', palette='viridis')

================================================================================
INFERENCE BENCHMARKING (SPEED & FLOPS)
================================================================================
File: Add to `inference.py` or create `benchmark.py`

Benchmark Function:
  def benchmark_inference(model, device, batch_size=128, n_warmup=20, n_iters=200):
      model.eval()
      param_dim, seq_len = model.hparams.model['param_dim'], model.hparams.dataset['pchip']['seq_len']
      X_dummy = torch.randn(batch_size, param_dim, device=device)
      V_dummy = torch.randn(batch_size, seq_len, device=device)

      # Warmup
      with torch.no_grad():
          for _ in range(n_warmup):
              _ = model(X_dummy, V_dummy)

      # Timing
      if device.type == 'cuda':
          starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
          starter.record()
          for _ in range(n_iters):
              _ = model(X_dummy, V_dummy)
          ender.record()
          torch.cuda.synchronize()
          total_ms = starter.elapsed_time(ender)
      else:
          t0 = time.perf_counter()
          for _ in range(n_iters):
              _ = model(X_dummy, V_dummy)
          total_ms = (time.perf_counter() - t0) * 1000.0

      avg_ms = total_ms / n_iters
      throughput = batch_size / (avg_ms / 1000)

      print(f"\nDevice: {device}  Batch size: {batch_size}")
      print(f"  → Total time for {n_iters} runs: {total_ms:.1f} ms")
      print(f"  → Average latency per forward: {avg_ms:.3f} ms")
      print(f"  → Throughput: {throughput:.0f} samples/s\n")

      return avg_ms, throughput

Usage:
  model = load_trained_model('models/ctrl_point_model.pt')
  benchmark_inference(model, device='cuda', batch_size=128)

FLOP Counting (optional):
  Use pytorch-flops or thop:
    from thop import profile
    flops, params = profile(model, inputs=(X_dummy, V_dummy))
    print(f"FLOPs: {flops/1e9:.2f}G, Params: {params/1e6:.2f}M")

================================================================================
KEY IMPLEMENTATION DETAILS FOR REPLICATION (UPDATED v3.0)
================================================================================
Voltage grid: `config.py` hardcodes `V_GRID` (45 points)
Control points: Default K=4 per region → 6 knots total per region
Cumulative scaling: build_knots uses cumsum for guaranteed monotonicity
PCHIP slopes: Fritsch-Carlson with eps=1e-12 safe division
Continuity: Finite differences with local grid spacing
Kendall init: log_sigma = torch.zeros(1) → σ=1.0 initially
Feature mask: Boolean array in checkpoint, applied before physics computation
Hard clamp training: Default True, fixes train-test distribution shift
Logging: TrainingLogger accumulates logs, saves all on completion

**NEW in v3.0:**
Curve normalization: 2.0 * (curve / isc) - 1.0 → [-1, 1] (decouples magnitude/shape)
Denormalization: (normalized + 1.0) / 2.0 * isc → [0, isc] (for evaluation)
Parameter transform: log1p → RobustScaler → MinMaxScaler → [-1, 1] (handles outliers)
Preprocessing class: PVDataPreprocessor (fit on train, transform val/test)
CRITICAL: Curve reconstruction uses PREDICTED anchors (not raw inputs) ✓ Correct
Training space: Normalized [-1, 1] for better gradients
Evaluation space: Absolute [0, isc] for interpretability
Validation: Round-trip error < 1e-4, normalized range [-1.1, 1.1]

================================================================================
END OF DOCUMENTATION (v3.0 - WITH DATA PREPROCESSING PIPELINE)
================================================================================
